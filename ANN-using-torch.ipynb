{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have been given a partially implemented code for a feed-forward neural network using PyTorch. Your task is to complete the missing parts of the code to make it functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "label_size = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "# Create the neural network object\n",
    "model = NeuralNetwork(input_size, hidden_size, label_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Generate some dummy data for training\n",
    "train_data = torch.randn(100, input_size)\n",
    "train_labels = torch.randint(label_size, (100,))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    # Complete this line to pass the training data through the model and obtain the predictions\n",
    "    outputs = model(train_data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, train_labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the trained model\n",
    "test_data = torch.randn(10, input_size)\n",
    "with torch.no_grad():\n",
    "    # Complete this line to pass the test data through the model and obtain the predictions\n",
    "    test_outputs = model(test_data)\n",
    "\n",
    "    # Print the predictions\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    print(\"Predictions:\", predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In this coding exercise, you need to implement the training of a deep MLP on the MNIST dataset using PyTorch and manually tune the hyperparameters. Follow the steps below to proceed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the MNIST dataset using torchvision.datasets.MNIST. The dataset contains handwritten digit images, and it can be easily accessed through PyTorch's torchvision module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "\n",
    "mean = 0.13066048920154572\n",
    "std = 0.30810779333114624\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean=(mean,), std=(std,))])\n",
    "\n",
    "train_mnist = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_mnist = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define your deep MLP model. Specify the number of hidden layers, the number of neurons in each layer, and the activation function to be used. You can use the nn.Sequential container to stack the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                              nn.Linear(784, 400),\n",
    "                              nn.SELU(),\n",
    "                              nn.Dropout(0.2),\n",
    "                              nn.Linear(400, 200),\n",
    "                              nn.SELU(),\n",
    "                              nn.Dropout(0.2),\n",
    "                              nn.Linear(200, 30),\n",
    "                              nn.SELU(),\n",
    "                              nn.Dropout(0.2),\n",
    "                              nn.Linear(30, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up the training loop and the hyperparameters. You can use the CrossEntropyLoss as the loss function and the Stochastic Gradient Descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "# Create data loaders\n",
    "train_dloader = DataLoader(train_mnist, batch_size=batch_size, shuffle=True)\n",
    "test_dloader = DataLoader(test_mnist, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model by iterating over the training dataset for the specified number of epochs. Compute the loss, perform backpropagation, and update the model's parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    model.train(True)\n",
    "\n",
    "    for images, labels in train_dloader:\n",
    "        # Flatten the images\n",
    "        images = images.view(-1, 784)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the trained model on the test dataset and calculate the accuracy (Please take a moment to consider the code below!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dloader:\n",
    "        images = images.view(-1, 784)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Manually tune the hyperparameters, such as the learning rate, by experimenting with different values and observing the performance. You can also search for the optimal learning rate by using techniques like learning rate range test, where you gradually increase the learning rate and monitor the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In this coding exercise, you'll have an opportunity to explore the behavior of a deep neural network trained on the CIFAR10 image dataset. Follow the steps below:\n",
    "\n",
    "* a. Construct a deep neural network (DNN) using 20 hidden layers, each comprising 100 neurons. To facilitate this exploration, employ the Swish activation function for each layer. Utilize nn.ModuleList to manage the layers effectively.\n",
    "\n",
    "* b. Load the CIFAR10 dataset for training your network. Utilize the appropriate function, such as torchvision.datasets.CIFAR10. The dataset consists of 60,000 color images, with dimensions of 32Ã—32 pixels. It is divided into 50,000 training samples and 10,000 testing samples. With 10 classes in the dataset, ensure that your network has a softmax output layer comprising 10 neurons. When modifying the model's architecture or hyperparameters, conduct a search to identify an appropriate learning rate. Implement early stopping during training and employ the Nadam optimization algorithm.\n",
    "\n",
    "* c. Experiment by adding batch normalization to your network. Compare the learning curves obtained with and without batch normalization. Analyze whether the model converges faster with batch normalization and observe any improvements in its performance. Additionally, assess the impact of batch normalization on training speed.\n",
    "\n",
    "* d. As an additional experiment, substitute batch normalization with SELU (Scaled Exponential Linear Units). Make the necessary adjustments to ensure the network self-normalizes. This involves standardizing the input features, initializing the network's weights using LeCun normal initialization (nn.init.kaiming_normal_), and ensuring that the DNN consists solely of dense layers. Observe the effects of utilizing SELU activation and self-normalization on the network's training stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "class MyNN1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MyNN1, self).__init__()\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(input_size, hidden_size))\n",
    "        hidden_layers.append(nn.SiLU())\n",
    "\n",
    "        for _ in range(19): \n",
    "            hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            hidden_layers.append(nn.SiLU())\n",
    "\n",
    "        hidden_layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.model = nn.Sequential(*hidden_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING DATASET\n",
    "transform_CIFAR10 = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_CIFAR10 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_CIFAR10)\n",
    "print(\"train size: \", len(train_CIFAR10))\n",
    "test_CIFAR10 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_CIFAR10)\n",
    "print(\"test size: \", len(test_CIFAR10))\n",
    "\n",
    "train_dloader = DataLoader(dataset=train_CIFAR10, batch_size=50, shuffle=True)\n",
    "test_dloader = DataLoader(dataset=test_CIFAR10, batch_size=50, shuffle=False)\n",
    "\n",
    "## SETTING HYPERPARAMETERS\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "input_size = 32*32*3\n",
    "output_size = 100\n",
    "num_class = 10\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyNN1(input_size,output_size,num_class)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in train_dloader:\n",
    "        images = inputs.view(inputs.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, labels in test_dloader:\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    epoch_loss = total_loss / len(test_dloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dloader:\n",
    "        images = images.view(inputs.size(0), -1)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN_WithBatch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MyNN_WithBatch, self).__init__()\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(input_size, hidden_size))\n",
    "        hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        hidden_layers.append(nn.SiLU())\n",
    "\n",
    "        for _ in range(19): \n",
    "            hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            hidden_layers.append(nn.SiLU())\n",
    "\n",
    "        hidden_layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.model = nn.Sequential(*hidden_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Batch = MyNN_WithBatch(input_size,output_size,num_class)\n",
    "for epoch in range(epochs): \n",
    "    model_Batch.train()\n",
    "\n",
    "    for inputs, labels in train_dloader:\n",
    "        images = inputs.view(inputs.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_Batch(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model_Batch.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, labels in test_dloader:\n",
    "            outputs = model_Batch(inputs.view(inputs.size(0), -1))\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    epoch_loss = total_loss / len(test_dloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN_WithSELU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MyNN_WithSELU, self).__init__()\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(input_size, hidden_size))\n",
    "        hidden_layers.append(nn.SELU(hidden_size))\n",
    "\n",
    "        for _ in range(19): \n",
    "            hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            hidden_layers.append(nn.SELU())\n",
    "\n",
    "        hidden_layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.model = nn.Sequential(*hidden_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Selu = MyNN_WithSELU(input_size,output_size,num_class)\n",
    "for epoch in range(epochs):  \n",
    "    model_Selu.train()\n",
    "\n",
    "    for inputs, labels in train_dloader:\n",
    "        images = inputs.view(inputs.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_Selu(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model_Selu.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, labels in test_dloader:\n",
    "            outputs = model_Selu(inputs.view(inputs.size(0), -1))\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    epoch_loss = total_loss / len(test_dloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
