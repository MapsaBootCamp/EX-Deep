{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have been given a partially implemented code for a feed-forward neural network using PyTorch. Your task is to complete the missing parts of the code to make it functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/1000, Loss: 1.609714150428772\n",
      "Epoch: 200/1000, Loss: 1.608396053314209\n",
      "Epoch: 300/1000, Loss: 1.6071648597717285\n",
      "Epoch: 400/1000, Loss: 1.605959415435791\n",
      "Epoch: 500/1000, Loss: 1.6047974824905396\n",
      "Epoch: 600/1000, Loss: 1.6036269664764404\n",
      "Epoch: 700/1000, Loss: 1.602494478225708\n",
      "Epoch: 800/1000, Loss: 1.6013919115066528\n",
      "Epoch: 900/1000, Loss: 1.600313425064087\n",
      "Epoch: 1000/1000, Loss: 1.5991991758346558\n",
      "Predictions: tensor([4, 2, 0, 2, 4, 2, 4, 2, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)# Complete this line to pass the output of the first layer through the activation function\n",
    "        x = self.relu(x)# Complete this line to pass the output of the activation function through the second layer\n",
    "        return x\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "label_size = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "# Create the neural network object\n",
    "model = NeuralNetwork(input_size, hidden_size, label_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Generate some dummy data for training\n",
    "train_data = torch.randn(100, input_size)\n",
    "train_labels = torch.randint(label_size, (100,))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    model.train(True)\n",
    "    # Complete this line to pass the training data through the model and obtain the predictions\n",
    "    outputs = model(train_data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, train_labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the trained model\n",
    "test_data = torch.randn(10, input_size)\n",
    "with torch.no_grad():\n",
    "    # Complete this line to pass the test data through the model and obtain the predictions\n",
    "    test_outputs = model(test_data)\n",
    "\n",
    "    # Print the predictions\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    print(\"Predictions:\", predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In this coding exercise, you need to implement the training of a deep MLP on the MNIST dataset using PyTorch and manually tune the hyperparameters. Follow the steps below to proceed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the MNIST dataset using torchvision.datasets.MNIST. The dataset contains handwritten digit images, and it can be easily accessed through PyTorch's torchvision module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\programming\\\\deeplearning\\\\exercise\\\\W2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:26<00:00, 368718.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist_data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 683386.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist_data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:03<00:00, 412567.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./mnist_data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./mnist_data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "train_mnist=datasets.MNIST(root='./mnist_data',train=True,download=True,transform=ToTensor())\n",
    "test_mnist=datasets.MNIST(root='./mnist_data',train=False,transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define your deep MLP model. Specify the number of hidden layers, the number of neurons in each layer, and the activation function to be used. You can use the nn.Sequential container to stack the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,in_feature:int,output_feature:int,*args,**kwargs):\n",
    "        super().__init__()\n",
    "        # self.flatten=nn.Flatten()\n",
    "        self.l1=nn.Linear(in_feature,100)\n",
    "        self.l2=nn.Linear(100,50)\n",
    "        self.l3=nn.Linear(50,output_feature)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x=self.flatten(x)\n",
    "        x=self.l1(x)\n",
    "        output=nn.ReLU()(x)\n",
    "        output=self.l2(output)\n",
    "        output=nn.ReLU()(output)\n",
    "        return self.l3(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up the training loop and the hyperparameters. You can use the CrossEntropyLoss as the loss function and the Stochastic Gradient Descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_mnist,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_mnist,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MLP(784,10)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model by iterating over the training dataset for the specified number of epochs. Compute the loss, perform backpropagation, and update the model's parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "tensor(0.0016, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "import tqdm\n",
    "for epoch in range(epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    model.train(True)\n",
    "    for batch_in,(images, labels)  in enumerate(train_loader): \n",
    "#         # Flatten the images\n",
    "        images = images.view(-1, 784)\n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs,labels)\n",
    "        \n",
    "\n",
    "#         # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "#         # Forward pass\n",
    "        optimizer.step()\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016163301188498735\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the trained model on the test dataset and calculate the accuracy (Please take a moment to consider the code below!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.98%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 784)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Manually tune the hyperparameters, such as the learning rate, by experimenting with different values and observing the performance. You can also search for the optimal learning rate by using techniques like learning rate range test, where you gradually increase the learning rate and monitor the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In this coding exercise, you'll have an opportunity to explore the behavior of a deep neural network trained on the CIFAR10 image dataset. Follow the steps below:\n",
    "\n",
    "* a. Construct a deep neural network (DNN) using 20 hidden layers, each comprising 100 neurons. To facilitate this exploration, employ the Swish activation function for each layer. Utilize nn.ModuleList to manage the layers effectively.\n",
    "\n",
    "* b. Load the CIFAR10 dataset for training your network. Utilize the appropriate function, such as torchvision.datasets.CIFAR10. The dataset consists of 60,000 color images, with dimensions of 32×32 pixels. It is divided into 50,000 training samples and 10,000 testing samples. With 10 classes in the dataset, ensure that your network has a softmax output layer comprising 10 neurons. When modifying the model's architecture or hyperparameters, conduct a search to identify an appropriate learning rate. Implement early stopping during training and employ the Nadam optimization algorithm.\n",
    "\n",
    "* c. Experiment by adding batch normalization to your network. Compare the learning curves obtained with and without batch normalization. Analyze whether the model converges faster with batch normalization and observe any improvements in its performance. Additionally, assess the impact of batch normalization on training speed.\n",
    "\n",
    "* d. As an additional experiment, substitute batch normalization with SELU (Scaled Exponential Linear Units). Make the necessary adjustments to ensure the network self-normalizes. This involves standardizing the input features, initializing the network's weights using LeCun normal initialization (nn.init.kaiming_normal_), and ensuring that the DNN consists solely of dense layers. Observe the effects of utilizing SELU activation and self-normalization on the network's training stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "class Cifar(nn.Module):\n",
    "    def __init__(self, input_size,output_size,hidden_layer_size,hidden_layer_amount) -> None:\n",
    "        super().__init__()\n",
    "        self.input_layer=nn.Linear(input_size,hidden_layer_size)\n",
    "        self.hidden_layer=nn.ModuleList()\n",
    "        self.output_layer=nn.Linear(hidden_layer_size,output_size)\n",
    "        self.swish=nn.SiLU()\n",
    "        self.softmax=nn.Softmax()\n",
    "        self.flatten=nn.Flatten()\n",
    "        for _ in range(hidden_layer_amount):\n",
    "            self.hidden_layer.append(nn.Linear(hidden_layer_size,hidden_layer_size))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.flatten(x)\n",
    "        output=self.input_layer(x)\n",
    "        output=self.swish(output)\n",
    "        for layer in self.hidden_layer:\n",
    "            output=layer(output)\n",
    "            output=self.swish(output)\n",
    "        output=self.output_layer(output)\n",
    "        output=self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train=datasets.CIFAR10(root='./Cfar10_data',train=True,download=True,transform=ToTensor())\n",
    "cifar_test=datasets.CIFAR10(root='./Cfar10_data',train=False,download=True,transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "input_size=32*32*3\n",
    "output_size=10\n",
    "train_dloader=DataLoader(cifar_train,batch_size=batch_size,shuffle=True)\n",
    "test_dloader=DataLoader(cifar_test,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = Cifar(input_size=input_size,output_size=output_size,hidden_layer_size=100,hidden_layer_amount=20)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programming\\deeplearning\\Deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0| train_loss: 2.302555799484253 | test_loss: 2.3031837940216064\n",
      "epoch: 1| train_loss: 2.3027424812316895 | test_loss: 2.3033342361450195\n",
      "epoch: 2| train_loss: 2.3028271198272705 | test_loss: 2.3028364181518555\n",
      "epoch: 3| train_loss: 2.30206561088562 | test_loss: 2.3021602630615234\n",
      "epoch: 4| train_loss: 2.3027660846710205 | test_loss: 2.301896095275879\n",
      "epoch: 5| train_loss: 2.302676200866699 | test_loss: 2.3021483421325684\n",
      "epoch: 6| train_loss: 2.3027963638305664 | test_loss: 2.302309036254883\n",
      "epoch: 7| train_loss: 2.302724838256836 | test_loss: 2.302990674972534\n",
      "epoch: 8| train_loss: 2.3023667335510254 | test_loss: 2.302685022354126\n",
      "epoch: 9| train_loss: 2.302807569503784 | test_loss: 2.302295446395874\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_in,(data,target) in enumerate(train_dloader):\n",
    "        images=data.view(data.size(0),-1)\n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data,test_target in test_dloader:\n",
    "            toutput=model(test_data)\n",
    "            tloss=criterion(toutput,test_target)\n",
    "    \n",
    "    print(f'epoch: {epoch}| train_loss: {loss.item()} | test_loss: {tloss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
