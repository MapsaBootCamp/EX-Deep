{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have been given a partially implemented code for a feed-forward neural network using PyTorch. Your task is to complete the missing parts of the code to make it functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/1000, Loss: 1.6116511821746826\n",
      "Epoch: 200/1000, Loss: 1.6108485460281372\n",
      "Epoch: 300/1000, Loss: 1.6100966930389404\n",
      "Epoch: 400/1000, Loss: 1.6093640327453613\n",
      "Epoch: 500/1000, Loss: 1.6086370944976807\n",
      "Epoch: 600/1000, Loss: 1.6079143285751343\n",
      "Epoch: 700/1000, Loss: 1.607240915298462\n",
      "Epoch: 800/1000, Loss: 1.606603980064392\n",
      "Epoch: 900/1000, Loss: 1.6060194969177246\n",
      "Epoch: 1000/1000, Loss: 1.6054507493972778\n",
      "Predictions: tensor([1, 0, 0, 1, 2, 1, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # Complete this line to pass the \n",
    "        #output of the first layer through the activation function\n",
    "        x = self.fc2(x)\n",
    "        # Complete this line to pass the \n",
    "        #output of the activation function through the second layer\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "label_size = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "# Create the neural network object\n",
    "model = NeuralNetwork(input_size, hidden_size, label_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Generate some dummy data for training\n",
    "train_data = torch.randn(100, input_size)\n",
    "train_labels = torch.randint(label_size, (100,))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    # Complete this line to pass the training data through the model and obtain the predictions\n",
    "    outputs = model(train_data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, train_labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the trained model\n",
    "test_data = torch.randn(10, input_size)\n",
    "with torch.no_grad():\n",
    "    # Complete this line to pass the test data through the model \n",
    "    #and obtain the predictions\n",
    "    test_outputs = model(test_data)\n",
    "\n",
    "    # Print the predictions\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    print(\"Predictions:\", predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In this coding exercise, you need to implement the training of a deep MLP on the MNIST dataset using PyTorch and manually tune the hyperparameters. Follow the steps below to proceed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the MNIST dataset using torchvision.datasets.MNIST. The dataset contains handwritten digit images, and it can be easily accessed through PyTorch's torchvision module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define your deep MLP model. Specify the number of hidden layers, the number of neurons in each layer, and the activation function to be used. You can use the nn.Sequential container to stack the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up the training loop and the hyperparameters. You can use the CrossEntropyLoss as the loss function and the Stochastic Gradient Descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'dict' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Define the loss function and optimizer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/sgd.py:13\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, lr\u001b[38;5;241m=\u001b[39mrequired, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dampening\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     11\u001b[0m              weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, maximize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, foreach: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m              differentiable: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m momentum \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'float'"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = {'learning_rate': [0.1, 0.01, 0.001, 0.0001]}\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Create data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model by iterating over the training dataset for the specified number of epochs. Compute the loss, perform backpropagation, and update the model's parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5400\n",
      "Epoch 2/10, Loss: 0.7378\n",
      "Epoch 3/10, Loss: 0.5383\n",
      "Epoch 4/10, Loss: 0.4604\n",
      "Epoch 5/10, Loss: 0.4186\n",
      "Epoch 6/10, Loss: 0.3919\n",
      "Epoch 7/10, Loss: 0.3732\n",
      "Epoch 8/10, Loss: 0.3590\n",
      "Epoch 9/10, Loss: 0.3478\n",
      "Epoch 10/10, Loss: 0.3385\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        # Flatten the images\n",
    "        images = images.view(-1, 784)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        if outputs is None:\n",
    "            raise ValueError(\"Model's forward pass returned None.\")\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the trained model on the test dataset and calculate the accuracy (Please take a moment to consider the code below!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.75%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.view(-1, 784)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Manually tune the hyperparameters, such as the learning rate, by experimenting with different values and observing the performance. You can also search for the optimal learning rate by using techniques like learning rate range test, where you gradually increase the learning rate and monitor the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'estimator' parameter of GridSearchCV must be an object implementing 'fit'. Got MLP(\n  (model): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=10, bias=True)\n    (3): LogSoftmax(dim=1)\n  )\n) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mtargets\n\u001b[1;32m      6\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1141\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1142\u001b[0m )\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1145\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'estimator' parameter of GridSearchCV must be an object implementing 'fit'. Got MLP(\n  (model): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=10, bias=True)\n    (3): LogSoftmax(dim=1)\n  )\n) instead."
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Flatten the images in the dataset\n",
    "X_train = train_dataset.data.view(-1, 784)\n",
    "y_train = train_dataset.targets\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters and its corresponding accuracy\n",
    "print(\"Best Learning Rate: {:.4f}\".format(grid_search.best_params_['learning_rate']))\n",
    "print(\"Best Accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In this coding exercise, you'll have an opportunity to explore the behavior of a deep neural network trained on the CIFAR10 image dataset. Follow the steps below:\n",
    "\n",
    "* a. Construct a deep neural network (DNN) using **20 hidden layers**, each comprising **100 neurons. ** To facilitate this exploration, employ the **Swish activation function** for each layer. Utilize nn.ModuleList to manage the layers effectively.\n",
    "\n",
    "* b. Load the CIFAR10 dataset for training your network. Utilize the appropriate function, such as **torchvision.datasets.CIFAR10**. The dataset consists of 60,000 color images, with dimensions of 32×32 pixels. It is divided into 50,000 training samples and 10,000 testing samples. With 10 classes in the dataset, ensure that your network has a **softmax output layer **comprising # 10 neurons. When modifying the model's architecture or hyperparameters, conduct a search to identify an appropriate  **learning rate**. Implement # early stopping during training and employ the # Nadam optimization algorithm.\n",
    "\n",
    "* c. Experiment by # adding batch normalization to your network. Compare the learning curves obtained with and without batch normalization. Analyze whether the model converges faster with batch normalization and observe any improvements in its performance. Additionally, assess the impact of batch normalization on training speed.\n",
    "\n",
    "* d. As an additional experiment, substitute batch normalization with SELU (Scaled Exponential Linear Units). Make the necessary adjustments to ensure the network self-normalizes. This involves standardizing the input features, initializing the network's weights using LeCun normal initialization (nn.init.kaiming_normal_), and ensuring that the DNN consists solely of dense layers. Observe the effects of utilizing SELU activation and self-normalization on the network's training stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load and normalize CIFAR10\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Deep Neural Network\n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(DeepNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(20):\n",
    "            self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "            self.layers.append(torch.nn.SiLU(inplace=False))\n",
    "            input_size = hidden_size\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "dnn = DeepNeuralNetwork(input_size=32*32*3, hidden_size=100, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function, optimizer, and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(model.parameters())\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.046\n",
      "[1,    51] loss: 2.302\n",
      "[1,   101] loss: 2.302\n",
      "[1,   151] loss: 2.299\n",
      "[1,   201] loss: 2.306\n",
      "[1,   251] loss: 2.300\n",
      "[1,   301] loss: 2.309\n",
      "[1,   351] loss: 2.303\n",
      "[1,   401] loss: 2.307\n",
      "[1,   451] loss: 2.301\n",
      "[1,   501] loss: 2.312\n",
      "[1,   551] loss: 2.309\n",
      "[1,   601] loss: 2.305\n",
      "[1,   651] loss: 2.305\n",
      "[1,   701] loss: 2.303\n",
      "[1,   751] loss: 2.297\n",
      "[1,   801] loss: 2.303\n",
      "[1,   851] loss: 2.304\n",
      "[1,   901] loss: 2.309\n",
      "[1,   951] loss: 2.304\n",
      "[1,  1001] loss: 2.306\n",
      "[1,  1051] loss: 2.303\n",
      "[1,  1101] loss: 2.306\n",
      "[1,  1151] loss: 2.303\n",
      "[1,  1201] loss: 2.302\n",
      "[1,  1251] loss: 2.300\n",
      "[1,  1301] loss: 2.310\n",
      "[1,  1351] loss: 2.300\n",
      "[1,  1401] loss: 2.306\n",
      "[1,  1451] loss: 2.305\n",
      "[1,  1501] loss: 2.308\n",
      "[1,  1551] loss: 2.303\n",
      "[1,  1601] loss: 2.300\n",
      "[1,  1651] loss: 2.301\n",
      "[1,  1701] loss: 2.305\n",
      "[1,  1751] loss: 2.297\n",
      "[1,  1801] loss: 2.303\n",
      "[1,  1851] loss: 2.298\n",
      "[1,  1901] loss: 2.304\n",
      "[1,  1951] loss: 2.301\n",
      "[1,  2001] loss: 2.309\n",
      "[1,  2051] loss: 2.305\n",
      "[1,  2101] loss: 2.301\n",
      "[1,  2151] loss: 2.305\n",
      "[1,  2201] loss: 2.300\n",
      "[1,  2251] loss: 2.304\n",
      "[1,  2301] loss: 2.298\n",
      "[1,  2351] loss: 2.306\n",
      "[1,  2401] loss: 2.303\n",
      "[1,  2451] loss: 2.307\n",
      "[1,  2501] loss: 2.305\n",
      "[1,  2551] loss: 2.308\n",
      "[1,  2601] loss: 2.301\n",
      "[1,  2651] loss: 2.299\n",
      "[1,  2701] loss: 2.305\n",
      "[1,  2751] loss: 2.306\n",
      "[1,  2801] loss: 2.306\n",
      "[1,  2851] loss: 2.301\n",
      "[1,  2901] loss: 2.301\n",
      "[1,  2951] loss: 2.304\n",
      "[1,  3001] loss: 2.305\n",
      "[1,  3051] loss: 2.303\n",
      "[1,  3101] loss: 2.300\n",
      "[1,  3151] loss: 2.308\n",
      "[1,  3201] loss: 2.305\n",
      "[1,  3251] loss: 2.302\n",
      "[1,  3301] loss: 2.309\n",
      "[1,  3351] loss: 2.304\n",
      "[1,  3401] loss: 2.309\n",
      "[1,  3451] loss: 2.312\n",
      "[1,  3501] loss: 2.310\n",
      "[1,  3551] loss: 2.306\n",
      "[1,  3601] loss: 2.303\n",
      "[1,  3651] loss: 2.301\n",
      "[1,  3701] loss: 2.304\n",
      "[1,  3751] loss: 2.307\n",
      "[1,  3801] loss: 2.301\n",
      "[1,  3851] loss: 2.309\n",
      "[1,  3901] loss: 2.306\n",
      "[1,  3951] loss: 2.307\n",
      "[1,  4001] loss: 2.301\n",
      "[1,  4051] loss: 2.302\n",
      "[1,  4101] loss: 2.304\n",
      "[1,  4151] loss: 2.302\n",
      "[1,  4201] loss: 2.304\n",
      "[1,  4251] loss: 2.300\n",
      "[1,  4301] loss: 2.302\n",
      "[1,  4351] loss: 2.307\n",
      "[1,  4401] loss: 2.304\n",
      "[1,  4451] loss: 2.301\n",
      "[1,  4501] loss: 2.307\n",
      "[1,  4551] loss: 2.308\n",
      "[1,  4601] loss: 2.305\n",
      "[1,  4651] loss: 2.301\n",
      "[1,  4701] loss: 2.303\n",
      "[1,  4751] loss: 2.304\n",
      "[1,  4801] loss: 2.307\n",
      "[1,  4851] loss: 2.305\n",
      "[1,  4901] loss: 2.306\n",
      "[1,  4951] loss: 2.303\n",
      "[1,  5001] loss: 2.296\n",
      "[1,  5051] loss: 2.307\n",
      "[1,  5101] loss: 2.300\n",
      "[1,  5151] loss: 2.305\n",
      "[1,  5201] loss: 2.310\n",
      "[1,  5251] loss: 2.300\n",
      "[1,  5301] loss: 2.299\n",
      "[1,  5351] loss: 2.299\n",
      "[1,  5401] loss: 2.300\n",
      "[1,  5451] loss: 2.305\n",
      "[1,  5501] loss: 2.304\n",
      "[1,  5551] loss: 2.306\n",
      "[1,  5601] loss: 2.305\n",
      "[1,  5651] loss: 2.309\n",
      "[1,  5701] loss: 2.301\n",
      "[1,  5751] loss: 2.299\n",
      "[1,  5801] loss: 2.302\n",
      "[1,  5851] loss: 2.301\n",
      "[1,  5901] loss: 2.305\n",
      "[1,  5951] loss: 2.308\n",
      "[1,  6001] loss: 2.304\n",
      "[1,  6051] loss: 2.304\n",
      "[1,  6101] loss: 2.299\n",
      "[1,  6151] loss: 2.303\n",
      "[1,  6201] loss: 2.303\n",
      "[1,  6251] loss: 2.301\n",
      "[1,  6301] loss: 2.299\n",
      "[1,  6351] loss: 2.302\n",
      "[1,  6401] loss: 2.305\n",
      "[1,  6451] loss: 2.304\n",
      "[1,  6501] loss: 2.305\n",
      "[1,  6551] loss: 2.303\n",
      "[1,  6601] loss: 2.303\n",
      "[1,  6651] loss: 2.302\n",
      "[1,  6701] loss: 2.307\n",
      "[1,  6751] loss: 2.309\n",
      "[1,  6801] loss: 2.300\n",
      "[1,  6851] loss: 2.298\n",
      "[1,  6901] loss: 2.302\n",
      "[1,  6951] loss: 2.301\n",
      "[1,  7001] loss: 2.300\n",
      "[1,  7051] loss: 2.301\n",
      "[1,  7101] loss: 2.306\n",
      "[1,  7151] loss: 2.306\n",
      "[1,  7201] loss: 2.306\n",
      "[1,  7251] loss: 2.303\n",
      "[1,  7301] loss: 2.302\n",
      "[1,  7351] loss: 2.304\n",
      "[1,  7401] loss: 2.303\n",
      "[1,  7451] loss: 2.303\n",
      "[1,  7501] loss: 2.303\n",
      "[1,  7551] loss: 2.303\n",
      "[1,  7601] loss: 2.303\n",
      "[1,  7651] loss: 2.299\n",
      "[1,  7701] loss: 2.305\n",
      "[1,  7751] loss: 2.305\n",
      "[1,  7801] loss: 2.303\n",
      "[1,  7851] loss: 2.303\n",
      "[1,  7901] loss: 2.302\n",
      "[1,  7951] loss: 2.301\n",
      "[1,  8001] loss: 2.303\n",
      "[1,  8051] loss: 2.307\n",
      "[1,  8101] loss: 2.300\n",
      "[1,  8151] loss: 2.310\n",
      "[1,  8201] loss: 2.309\n",
      "[1,  8251] loss: 2.308\n",
      "[1,  8301] loss: 2.306\n",
      "[1,  8351] loss: 2.307\n",
      "[1,  8401] loss: 2.303\n",
      "[1,  8451] loss: 2.301\n",
      "[1,  8501] loss: 2.305\n",
      "[1,  8551] loss: 2.304\n",
      "[1,  8601] loss: 2.303\n",
      "[1,  8651] loss: 2.301\n",
      "[1,  8701] loss: 2.310\n",
      "[1,  8751] loss: 2.307\n",
      "[1,  8801] loss: 2.306\n",
      "[1,  8851] loss: 2.306\n",
      "[1,  8901] loss: 2.305\n",
      "[1,  8951] loss: 2.310\n",
      "[1,  9001] loss: 2.304\n",
      "[1,  9051] loss: 2.299\n",
      "[1,  9101] loss: 2.312\n",
      "[1,  9151] loss: 2.299\n",
      "[1,  9201] loss: 2.305\n",
      "[1,  9251] loss: 2.301\n",
      "[1,  9301] loss: 2.303\n",
      "[1,  9351] loss: 2.303\n",
      "[1,  9401] loss: 2.302\n",
      "[1,  9451] loss: 2.302\n",
      "[1,  9501] loss: 2.301\n",
      "[1,  9551] loss: 2.305\n",
      "[1,  9601] loss: 2.303\n",
      "[1,  9651] loss: 2.300\n",
      "[1,  9701] loss: 2.302\n",
      "[1,  9751] loss: 2.302\n",
      "[1,  9801] loss: 2.308\n",
      "[1,  9851] loss: 2.302\n",
      "[1,  9901] loss: 2.298\n",
      "[1,  9951] loss: 2.303\n",
      "[1, 10001] loss: 2.307\n",
      "[1, 10051] loss: 2.306\n",
      "[1, 10101] loss: 2.307\n",
      "[1, 10151] loss: 2.299\n",
      "[1, 10201] loss: 2.303\n",
      "[1, 10251] loss: 2.303\n",
      "[1, 10301] loss: 2.302\n",
      "[1, 10351] loss: 2.300\n",
      "[1, 10401] loss: 2.305\n",
      "[1, 10451] loss: 2.305\n",
      "[1, 10501] loss: 2.301\n",
      "[1, 10551] loss: 2.299\n",
      "[1, 10601] loss: 2.305\n",
      "[1, 10651] loss: 2.302\n",
      "[1, 10701] loss: 2.305\n",
      "[1, 10751] loss: 2.306\n",
      "[1, 10801] loss: 2.307\n",
      "[1, 10851] loss: 2.308\n",
      "[1, 10901] loss: 2.298\n",
      "[1, 10951] loss: 2.301\n",
      "[1, 11001] loss: 2.302\n",
      "[1, 11051] loss: 2.309\n",
      "[1, 11101] loss: 2.306\n",
      "[1, 11151] loss: 2.303\n",
      "[1, 11201] loss: 2.300\n",
      "[1, 11251] loss: 2.302\n",
      "[1, 11301] loss: 2.303\n",
      "[1, 11351] loss: 2.298\n",
      "[1, 11401] loss: 2.302\n",
      "[1, 11451] loss: 2.302\n",
      "[1, 11501] loss: 2.302\n",
      "[1, 11551] loss: 2.298\n",
      "[1, 11601] loss: 2.307\n",
      "[1, 11651] loss: 2.298\n",
      "[1, 11701] loss: 2.301\n",
      "[1, 11751] loss: 2.305\n",
      "[1, 11801] loss: 2.300\n",
      "[1, 11851] loss: 2.305\n",
      "[1, 11901] loss: 2.313\n",
      "[1, 11951] loss: 2.304\n",
      "[1, 12001] loss: 2.302\n",
      "[1, 12051] loss: 2.304\n",
      "[1, 12101] loss: 2.303\n",
      "[1, 12151] loss: 2.304\n",
      "[1, 12201] loss: 2.301\n",
      "[1, 12251] loss: 2.302\n",
      "[1, 12301] loss: 2.303\n",
      "[1, 12351] loss: 2.304\n",
      "[1, 12401] loss: 2.304\n",
      "[1, 12451] loss: 2.298\n",
      "[2,     1] loss: 0.046\n",
      "[2,    51] loss: 2.297\n",
      "[2,   101] loss: 2.300\n",
      "[2,   151] loss: 2.300\n",
      "[2,   201] loss: 2.307\n",
      "[2,   251] loss: 2.304\n",
      "[2,   301] loss: 2.303\n",
      "[2,   351] loss: 2.304\n",
      "[2,   401] loss: 2.301\n",
      "[2,   451] loss: 2.297\n",
      "[2,   501] loss: 2.304\n",
      "[2,   551] loss: 2.304\n",
      "[2,   601] loss: 2.307\n",
      "[2,   651] loss: 2.299\n",
      "[2,   701] loss: 2.298\n",
      "[2,   751] loss: 2.304\n",
      "[2,   801] loss: 2.302\n",
      "[2,   851] loss: 2.309\n",
      "[2,   901] loss: 2.299\n",
      "[2,   951] loss: 2.307\n",
      "[2,  1001] loss: 2.299\n",
      "[2,  1051] loss: 2.309\n",
      "[2,  1101] loss: 2.307\n",
      "[2,  1151] loss: 2.306\n",
      "[2,  1201] loss: 2.311\n",
      "[2,  1251] loss: 2.299\n",
      "[2,  1301] loss: 2.300\n",
      "[2,  1351] loss: 2.304\n",
      "[2,  1401] loss: 2.310\n",
      "[2,  1451] loss: 2.305\n",
      "[2,  1501] loss: 2.306\n",
      "[2,  1551] loss: 2.306\n",
      "[2,  1601] loss: 2.305\n",
      "[2,  1651] loss: 2.303\n",
      "[2,  1701] loss: 2.299\n",
      "[2,  1751] loss: 2.305\n",
      "[2,  1801] loss: 2.307\n",
      "[2,  1851] loss: 2.312\n",
      "[2,  1901] loss: 2.305\n",
      "[2,  1951] loss: 2.305\n",
      "[2,  2001] loss: 2.308\n",
      "[2,  2051] loss: 2.308\n",
      "[2,  2101] loss: 2.306\n",
      "[2,  2151] loss: 2.299\n",
      "[2,  2201] loss: 2.310\n",
      "[2,  2251] loss: 2.301\n",
      "[2,  2301] loss: 2.308\n",
      "[2,  2351] loss: 2.304\n",
      "[2,  2401] loss: 2.301\n",
      "[2,  2451] loss: 2.305\n",
      "[2,  2501] loss: 2.302\n",
      "[2,  2551] loss: 2.298\n",
      "[2,  2601] loss: 2.303\n",
      "[2,  2651] loss: 2.303\n",
      "[2,  2701] loss: 2.304\n",
      "[2,  2751] loss: 2.299\n",
      "[2,  2801] loss: 2.304\n",
      "[2,  2851] loss: 2.306\n",
      "[2,  2901] loss: 2.305\n",
      "[2,  2951] loss: 2.309\n",
      "[2,  3001] loss: 2.308\n",
      "[2,  3051] loss: 2.306\n",
      "[2,  3101] loss: 2.300\n",
      "[2,  3151] loss: 2.304\n",
      "[2,  3201] loss: 2.300\n",
      "[2,  3251] loss: 2.307\n",
      "[2,  3301] loss: 2.303\n",
      "[2,  3351] loss: 2.305\n",
      "[2,  3401] loss: 2.305\n",
      "[2,  3451] loss: 2.303\n",
      "[2,  3501] loss: 2.305\n",
      "[2,  3551] loss: 2.297\n",
      "[2,  3601] loss: 2.304\n",
      "[2,  3651] loss: 2.299\n",
      "[2,  3701] loss: 2.303\n",
      "[2,  3751] loss: 2.304\n",
      "[2,  3801] loss: 2.304\n",
      "[2,  3851] loss: 2.303\n",
      "[2,  3901] loss: 2.306\n",
      "[2,  3951] loss: 2.307\n",
      "[2,  4001] loss: 2.305\n",
      "[2,  4051] loss: 2.307\n",
      "[2,  4101] loss: 2.304\n",
      "[2,  4151] loss: 2.302\n",
      "[2,  4201] loss: 2.306\n",
      "[2,  4251] loss: 2.309\n",
      "[2,  4301] loss: 2.305\n",
      "[2,  4351] loss: 2.299\n",
      "[2,  4401] loss: 2.307\n",
      "[2,  4451] loss: 2.300\n",
      "[2,  4501] loss: 2.299\n",
      "[2,  4551] loss: 2.307\n",
      "[2,  4601] loss: 2.301\n",
      "[2,  4651] loss: 2.301\n",
      "[2,  4701] loss: 2.300\n",
      "[2,  4751] loss: 2.298\n",
      "[2,  4801] loss: 2.305\n",
      "[2,  4851] loss: 2.306\n",
      "[2,  4901] loss: 2.304\n",
      "[2,  4951] loss: 2.308\n",
      "[2,  5001] loss: 2.297\n",
      "[2,  5051] loss: 2.311\n",
      "[2,  5101] loss: 2.307\n",
      "[2,  5151] loss: 2.303\n",
      "[2,  5201] loss: 2.301\n",
      "[2,  5251] loss: 2.312\n",
      "[2,  5301] loss: 2.305\n",
      "[2,  5351] loss: 2.306\n",
      "[2,  5401] loss: 2.303\n",
      "[2,  5451] loss: 2.304\n",
      "[2,  5501] loss: 2.301\n",
      "[2,  5551] loss: 2.305\n",
      "[2,  5601] loss: 2.303\n",
      "[2,  5651] loss: 2.306\n",
      "[2,  5701] loss: 2.306\n",
      "[2,  5751] loss: 2.301\n",
      "[2,  5801] loss: 2.304\n",
      "[2,  5851] loss: 2.304\n",
      "[2,  5901] loss: 2.307\n",
      "[2,  5951] loss: 2.306\n",
      "[2,  6001] loss: 2.306\n",
      "[2,  6051] loss: 2.301\n",
      "[2,  6101] loss: 2.302\n",
      "[2,  6151] loss: 2.302\n",
      "[2,  6201] loss: 2.299\n",
      "[2,  6251] loss: 2.308\n",
      "[2,  6301] loss: 2.300\n",
      "[2,  6351] loss: 2.305\n",
      "[2,  6401] loss: 2.305\n",
      "[2,  6451] loss: 2.301\n",
      "[2,  6501] loss: 2.308\n",
      "[2,  6551] loss: 2.306\n",
      "[2,  6601] loss: 2.297\n",
      "[2,  6651] loss: 2.303\n",
      "[2,  6701] loss: 2.302\n",
      "[2,  6751] loss: 2.307\n",
      "[2,  6801] loss: 2.300\n",
      "[2,  6851] loss: 2.309\n",
      "[2,  6901] loss: 2.303\n",
      "[2,  6951] loss: 2.307\n",
      "[2,  7001] loss: 2.308\n",
      "[2,  7051] loss: 2.306\n",
      "[2,  7101] loss: 2.304\n",
      "[2,  7151] loss: 2.306\n",
      "[2,  7201] loss: 2.305\n",
      "[2,  7251] loss: 2.300\n",
      "[2,  7301] loss: 2.303\n",
      "[2,  7351] loss: 2.301\n",
      "[2,  7401] loss: 2.302\n",
      "[2,  7451] loss: 2.306\n",
      "[2,  7501] loss: 2.303\n",
      "[2,  7551] loss: 2.303\n",
      "[2,  7601] loss: 2.305\n",
      "[2,  7651] loss: 2.307\n",
      "[2,  7701] loss: 2.304\n",
      "[2,  7751] loss: 2.297\n",
      "[2,  7801] loss: 2.304\n",
      "[2,  7851] loss: 2.297\n",
      "[2,  7901] loss: 2.300\n",
      "[2,  7951] loss: 2.306\n",
      "[2,  8001] loss: 2.308\n",
      "[2,  8051] loss: 2.304\n",
      "[2,  8101] loss: 2.304\n",
      "[2,  8151] loss: 2.306\n",
      "[2,  8201] loss: 2.309\n",
      "[2,  8251] loss: 2.305\n",
      "[2,  8301] loss: 2.309\n",
      "[2,  8351] loss: 2.302\n",
      "[2,  8401] loss: 2.300\n",
      "[2,  8451] loss: 2.302\n",
      "[2,  8501] loss: 2.302\n",
      "[2,  8551] loss: 2.304\n",
      "[2,  8601] loss: 2.301\n",
      "[2,  8651] loss: 2.302\n",
      "[2,  8701] loss: 2.301\n",
      "[2,  8751] loss: 2.304\n",
      "[2,  8801] loss: 2.308\n",
      "[2,  8851] loss: 2.297\n",
      "[2,  8901] loss: 2.300\n",
      "[2,  8951] loss: 2.304\n",
      "[2,  9001] loss: 2.305\n",
      "[2,  9051] loss: 2.300\n",
      "[2,  9101] loss: 2.296\n",
      "[2,  9151] loss: 2.302\n",
      "[2,  9201] loss: 2.297\n",
      "[2,  9251] loss: 2.307\n",
      "[2,  9301] loss: 2.305\n",
      "[2,  9351] loss: 2.304\n",
      "[2,  9401] loss: 2.309\n",
      "[2,  9451] loss: 2.301\n",
      "[2,  9501] loss: 2.299\n",
      "[2,  9551] loss: 2.296\n",
      "[2,  9601] loss: 2.300\n",
      "[2,  9651] loss: 2.302\n",
      "[2,  9701] loss: 2.299\n",
      "[2,  9751] loss: 2.306\n",
      "[2,  9801] loss: 2.301\n",
      "[2,  9851] loss: 2.308\n",
      "[2,  9901] loss: 2.309\n",
      "[2,  9951] loss: 2.301\n",
      "[2, 10001] loss: 2.307\n",
      "[2, 10051] loss: 2.310\n",
      "[2, 10101] loss: 2.302\n",
      "[2, 10151] loss: 2.301\n",
      "[2, 10201] loss: 2.306\n",
      "[2, 10251] loss: 2.302\n",
      "[2, 10301] loss: 2.305\n",
      "[2, 10351] loss: 2.300\n",
      "[2, 10401] loss: 2.309\n",
      "[2, 10451] loss: 2.303\n",
      "[2, 10501] loss: 2.301\n",
      "[2, 10551] loss: 2.301\n",
      "[2, 10601] loss: 2.300\n",
      "[2, 10651] loss: 2.301\n",
      "[2, 10701] loss: 2.306\n",
      "[2, 10751] loss: 2.297\n",
      "[2, 10801] loss: 2.302\n",
      "[2, 10851] loss: 2.304\n",
      "[2, 10901] loss: 2.307\n",
      "[2, 10951] loss: 2.305\n",
      "[2, 11001] loss: 2.301\n",
      "[2, 11051] loss: 2.306\n",
      "[2, 11101] loss: 2.306\n",
      "[2, 11151] loss: 2.298\n",
      "[2, 11201] loss: 2.306\n",
      "[2, 11251] loss: 2.303\n",
      "[2, 11301] loss: 2.303\n",
      "[2, 11351] loss: 2.300\n",
      "[2, 11401] loss: 2.306\n",
      "[2, 11451] loss: 2.307\n",
      "[2, 11501] loss: 2.305\n",
      "[2, 11551] loss: 2.304\n",
      "[2, 11601] loss: 2.300\n",
      "[2, 11651] loss: 2.303\n",
      "[2, 11701] loss: 2.301\n",
      "[2, 11751] loss: 2.301\n",
      "[2, 11801] loss: 2.307\n",
      "[2, 11851] loss: 2.306\n",
      "[2, 11901] loss: 2.311\n",
      "[2, 11951] loss: 2.299\n",
      "[2, 12001] loss: 2.302\n",
      "[2, 12051] loss: 2.309\n",
      "[2, 12101] loss: 2.301\n",
      "[2, 12151] loss: 2.302\n",
      "[2, 12201] loss: 2.300\n",
      "[2, 12251] loss: 2.306\n",
      "[2, 12301] loss: 2.302\n",
      "[2, 12351] loss: 2.308\n",
      "[2, 12401] loss: 2.302\n",
      "[2, 12451] loss: 2.301\n",
      "[3,     1] loss: 0.046\n",
      "[3,    51] loss: 2.307\n",
      "[3,   101] loss: 2.306\n",
      "[3,   151] loss: 2.304\n",
      "[3,   201] loss: 2.307\n",
      "[3,   251] loss: 2.302\n",
      "[3,   301] loss: 2.304\n",
      "[3,   351] loss: 2.300\n",
      "[3,   401] loss: 2.299\n",
      "[3,   451] loss: 2.297\n",
      "[3,   501] loss: 2.298\n",
      "[3,   551] loss: 2.300\n",
      "[3,   601] loss: 2.298\n",
      "[3,   651] loss: 2.300\n",
      "[3,   701] loss: 2.308\n",
      "[3,   751] loss: 2.300\n",
      "[3,   801] loss: 2.302\n",
      "[3,   851] loss: 2.302\n",
      "[3,   901] loss: 2.309\n",
      "[3,   951] loss: 2.304\n",
      "[3,  1001] loss: 2.298\n",
      "[3,  1051] loss: 2.307\n",
      "[3,  1101] loss: 2.308\n",
      "[3,  1151] loss: 2.306\n",
      "[3,  1201] loss: 2.304\n",
      "[3,  1251] loss: 2.303\n",
      "[3,  1301] loss: 2.304\n",
      "[3,  1351] loss: 2.297\n",
      "[3,  1401] loss: 2.298\n",
      "[3,  1451] loss: 2.304\n",
      "[3,  1501] loss: 2.301\n",
      "[3,  1551] loss: 2.304\n",
      "[3,  1601] loss: 2.300\n",
      "[3,  1651] loss: 2.303\n",
      "[3,  1701] loss: 2.299\n",
      "[3,  1751] loss: 2.304\n",
      "[3,  1801] loss: 2.313\n",
      "[3,  1851] loss: 2.304\n",
      "[3,  1901] loss: 2.303\n",
      "[3,  1951] loss: 2.302\n",
      "[3,  2001] loss: 2.307\n",
      "[3,  2051] loss: 2.306\n",
      "[3,  2101] loss: 2.306\n",
      "[3,  2151] loss: 2.300\n",
      "[3,  2201] loss: 2.306\n",
      "[3,  2251] loss: 2.304\n",
      "[3,  2301] loss: 2.298\n",
      "[3,  2351] loss: 2.300\n",
      "[3,  2401] loss: 2.302\n",
      "[3,  2451] loss: 2.306\n",
      "[3,  2501] loss: 2.303\n",
      "[3,  2551] loss: 2.303\n",
      "[3,  2601] loss: 2.305\n",
      "[3,  2651] loss: 2.305\n",
      "[3,  2701] loss: 2.299\n",
      "[3,  2751] loss: 2.305\n",
      "[3,  2801] loss: 2.305\n",
      "[3,  2851] loss: 2.310\n",
      "[3,  2901] loss: 2.303\n",
      "[3,  2951] loss: 2.302\n",
      "[3,  3001] loss: 2.302\n",
      "[3,  3051] loss: 2.302\n",
      "[3,  3101] loss: 2.303\n",
      "[3,  3151] loss: 2.309\n",
      "[3,  3201] loss: 2.304\n",
      "[3,  3251] loss: 2.308\n",
      "[3,  3301] loss: 2.301\n",
      "[3,  3351] loss: 2.300\n",
      "[3,  3401] loss: 2.302\n",
      "[3,  3451] loss: 2.303\n",
      "[3,  3501] loss: 2.308\n",
      "[3,  3551] loss: 2.308\n",
      "[3,  3601] loss: 2.303\n",
      "[3,  3651] loss: 2.299\n",
      "[3,  3701] loss: 2.304\n",
      "[3,  3751] loss: 2.308\n",
      "[3,  3801] loss: 2.305\n",
      "[3,  3851] loss: 2.305\n",
      "[3,  3901] loss: 2.296\n",
      "[3,  3951] loss: 2.301\n",
      "[3,  4001] loss: 2.303\n",
      "[3,  4051] loss: 2.302\n",
      "[3,  4101] loss: 2.305\n",
      "[3,  4151] loss: 2.312\n",
      "[3,  4201] loss: 2.306\n",
      "[3,  4251] loss: 2.304\n",
      "[3,  4301] loss: 2.308\n",
      "[3,  4351] loss: 2.301\n",
      "[3,  4401] loss: 2.301\n",
      "[3,  4451] loss: 2.305\n",
      "[3,  4501] loss: 2.308\n",
      "[3,  4551] loss: 2.305\n",
      "[3,  4601] loss: 2.304\n",
      "[3,  4651] loss: 2.302\n",
      "[3,  4701] loss: 2.297\n",
      "[3,  4751] loss: 2.302\n",
      "[3,  4801] loss: 2.306\n",
      "[3,  4851] loss: 2.306\n",
      "[3,  4901] loss: 2.302\n",
      "[3,  4951] loss: 2.304\n",
      "[3,  5001] loss: 2.307\n",
      "[3,  5051] loss: 2.303\n",
      "[3,  5101] loss: 2.309\n",
      "[3,  5151] loss: 2.302\n",
      "[3,  5201] loss: 2.304\n",
      "[3,  5251] loss: 2.302\n",
      "[3,  5301] loss: 2.310\n",
      "[3,  5351] loss: 2.304\n",
      "[3,  5401] loss: 2.307\n",
      "[3,  5451] loss: 2.308\n",
      "[3,  5501] loss: 2.299\n",
      "[3,  5551] loss: 2.308\n",
      "[3,  5601] loss: 2.300\n",
      "[3,  5651] loss: 2.301\n",
      "[3,  5701] loss: 2.302\n",
      "[3,  5751] loss: 2.305\n",
      "[3,  5801] loss: 2.306\n",
      "[3,  5851] loss: 2.307\n",
      "[3,  5901] loss: 2.304\n",
      "[3,  5951] loss: 2.303\n",
      "[3,  6001] loss: 2.305\n",
      "[3,  6051] loss: 2.307\n",
      "[3,  6101] loss: 2.311\n",
      "[3,  6151] loss: 2.306\n",
      "[3,  6201] loss: 2.305\n",
      "[3,  6251] loss: 2.298\n",
      "[3,  6301] loss: 2.306\n",
      "[3,  6351] loss: 2.306\n",
      "[3,  6401] loss: 2.305\n",
      "[3,  6451] loss: 2.302\n",
      "[3,  6501] loss: 2.305\n",
      "[3,  6551] loss: 2.303\n",
      "[3,  6601] loss: 2.302\n",
      "[3,  6651] loss: 2.305\n",
      "[3,  6701] loss: 2.301\n",
      "[3,  6751] loss: 2.303\n",
      "[3,  6801] loss: 2.304\n",
      "[3,  6851] loss: 2.307\n",
      "[3,  6901] loss: 2.306\n",
      "[3,  6951] loss: 2.306\n",
      "[3,  7001] loss: 2.300\n",
      "[3,  7051] loss: 2.307\n",
      "[3,  7101] loss: 2.303\n",
      "[3,  7151] loss: 2.300\n",
      "[3,  7201] loss: 2.306\n",
      "[3,  7251] loss: 2.304\n",
      "[3,  7301] loss: 2.306\n",
      "[3,  7351] loss: 2.302\n",
      "[3,  7401] loss: 2.303\n",
      "[3,  7451] loss: 2.303\n",
      "[3,  7501] loss: 2.301\n",
      "[3,  7551] loss: 2.302\n",
      "[3,  7601] loss: 2.306\n",
      "[3,  7651] loss: 2.303\n",
      "[3,  7701] loss: 2.298\n",
      "[3,  7751] loss: 2.304\n",
      "[3,  7801] loss: 2.309\n",
      "[3,  7851] loss: 2.304\n",
      "[3,  7901] loss: 2.303\n",
      "[3,  7951] loss: 2.300\n",
      "[3,  8001] loss: 2.300\n",
      "[3,  8051] loss: 2.311\n",
      "[3,  8101] loss: 2.300\n",
      "[3,  8151] loss: 2.301\n",
      "[3,  8201] loss: 2.295\n",
      "[3,  8251] loss: 2.303\n",
      "[3,  8301] loss: 2.305\n",
      "[3,  8351] loss: 2.309\n",
      "[3,  8401] loss: 2.306\n",
      "[3,  8451] loss: 2.306\n",
      "[3,  8501] loss: 2.301\n",
      "[3,  8551] loss: 2.308\n",
      "[3,  8601] loss: 2.302\n",
      "[3,  8651] loss: 2.306\n",
      "[3,  8701] loss: 2.309\n",
      "[3,  8751] loss: 2.300\n",
      "[3,  8801] loss: 2.302\n",
      "[3,  8851] loss: 2.307\n",
      "[3,  8901] loss: 2.308\n",
      "[3,  8951] loss: 2.301\n",
      "[3,  9001] loss: 2.300\n",
      "[3,  9051] loss: 2.303\n",
      "[3,  9101] loss: 2.302\n",
      "[3,  9151] loss: 2.308\n",
      "[3,  9201] loss: 2.306\n",
      "[3,  9251] loss: 2.300\n",
      "[3,  9301] loss: 2.303\n",
      "[3,  9351] loss: 2.305\n",
      "[3,  9401] loss: 2.300\n",
      "[3,  9451] loss: 2.304\n",
      "[3,  9501] loss: 2.304\n",
      "[3,  9551] loss: 2.301\n",
      "[3,  9601] loss: 2.310\n",
      "[3,  9651] loss: 2.308\n",
      "[3,  9701] loss: 2.302\n",
      "[3,  9751] loss: 2.303\n",
      "[3,  9801] loss: 2.303\n",
      "[3,  9851] loss: 2.304\n",
      "[3,  9901] loss: 2.298\n",
      "[3,  9951] loss: 2.308\n",
      "[3, 10001] loss: 2.301\n",
      "[3, 10051] loss: 2.307\n",
      "[3, 10101] loss: 2.299\n",
      "[3, 10151] loss: 2.306\n",
      "[3, 10201] loss: 2.298\n",
      "[3, 10251] loss: 2.299\n",
      "[3, 10301] loss: 2.310\n",
      "[3, 10351] loss: 2.305\n",
      "[3, 10401] loss: 2.305\n",
      "[3, 10451] loss: 2.303\n",
      "[3, 10501] loss: 2.307\n",
      "[3, 10551] loss: 2.301\n",
      "[3, 10601] loss: 2.297\n",
      "[3, 10651] loss: 2.306\n",
      "[3, 10701] loss: 2.305\n",
      "[3, 10751] loss: 2.307\n",
      "[3, 10801] loss: 2.310\n",
      "[3, 10851] loss: 2.298\n",
      "[3, 10901] loss: 2.306\n",
      "[3, 10951] loss: 2.305\n",
      "[3, 11001] loss: 2.298\n",
      "[3, 11051] loss: 2.302\n",
      "[3, 11101] loss: 2.305\n",
      "[3, 11151] loss: 2.307\n",
      "[3, 11201] loss: 2.301\n",
      "[3, 11251] loss: 2.305\n",
      "[3, 11301] loss: 2.305\n",
      "[3, 11351] loss: 2.296\n",
      "[3, 11401] loss: 2.305\n",
      "[3, 11451] loss: 2.306\n",
      "[3, 11501] loss: 2.302\n",
      "[3, 11551] loss: 2.303\n",
      "[3, 11601] loss: 2.299\n",
      "[3, 11651] loss: 2.304\n",
      "[3, 11701] loss: 2.303\n",
      "[3, 11751] loss: 2.309\n",
      "[3, 11801] loss: 2.304\n",
      "[3, 11851] loss: 2.302\n",
      "[3, 11901] loss: 2.305\n",
      "[3, 11951] loss: 2.299\n",
      "[3, 12001] loss: 2.307\n",
      "[3, 12051] loss: 2.307\n",
      "[3, 12101] loss: 2.302\n",
      "[3, 12151] loss: 2.299\n",
      "[3, 12201] loss: 2.304\n",
      "[3, 12251] loss: 2.303\n",
      "[3, 12301] loss: 2.301\n",
      "[3, 12351] loss: 2.305\n",
      "[3, 12401] loss: 2.306\n",
      "[3, 12451] loss: 2.305\n",
      "[4,     1] loss: 0.046\n",
      "[4,    51] loss: 2.303\n",
      "[4,   101] loss: 2.308\n",
      "[4,   151] loss: 2.308\n",
      "[4,   201] loss: 2.300\n",
      "[4,   251] loss: 2.303\n",
      "[4,   301] loss: 2.309\n",
      "[4,   351] loss: 2.301\n",
      "[4,   401] loss: 2.301\n",
      "[4,   451] loss: 2.299\n",
      "[4,   501] loss: 2.312\n",
      "[4,   551] loss: 2.302\n",
      "[4,   601] loss: 2.300\n",
      "[4,   651] loss: 2.301\n",
      "[4,   701] loss: 2.305\n",
      "[4,   751] loss: 2.301\n",
      "[4,   801] loss: 2.302\n",
      "[4,   851] loss: 2.303\n",
      "[4,   901] loss: 2.306\n",
      "[4,   951] loss: 2.303\n",
      "[4,  1001] loss: 2.300\n",
      "[4,  1051] loss: 2.309\n",
      "[4,  1101] loss: 2.301\n",
      "[4,  1151] loss: 2.301\n",
      "[4,  1201] loss: 2.305\n",
      "[4,  1251] loss: 2.306\n",
      "[4,  1301] loss: 2.303\n",
      "[4,  1351] loss: 2.311\n",
      "[4,  1401] loss: 2.303\n",
      "[4,  1451] loss: 2.307\n",
      "[4,  1501] loss: 2.305\n",
      "[4,  1551] loss: 2.301\n",
      "[4,  1601] loss: 2.298\n",
      "[4,  1651] loss: 2.302\n",
      "[4,  1701] loss: 2.303\n",
      "[4,  1751] loss: 2.304\n",
      "[4,  1801] loss: 2.305\n",
      "[4,  1851] loss: 2.303\n",
      "[4,  1901] loss: 2.302\n",
      "[4,  1951] loss: 2.304\n",
      "[4,  2001] loss: 2.303\n",
      "[4,  2051] loss: 2.301\n",
      "[4,  2101] loss: 2.308\n",
      "[4,  2151] loss: 2.308\n",
      "[4,  2201] loss: 2.303\n",
      "[4,  2251] loss: 2.312\n",
      "[4,  2301] loss: 2.305\n",
      "[4,  2351] loss: 2.302\n",
      "[4,  2401] loss: 2.300\n",
      "[4,  2451] loss: 2.304\n",
      "[4,  2501] loss: 2.308\n",
      "[4,  2551] loss: 2.306\n",
      "[4,  2601] loss: 2.301\n",
      "[4,  2651] loss: 2.308\n",
      "[4,  2701] loss: 2.302\n",
      "[4,  2751] loss: 2.308\n",
      "[4,  2801] loss: 2.308\n",
      "[4,  2851] loss: 2.303\n",
      "[4,  2901] loss: 2.302\n",
      "[4,  2951] loss: 2.304\n",
      "[4,  3001] loss: 2.307\n",
      "[4,  3051] loss: 2.311\n",
      "[4,  3101] loss: 2.300\n",
      "[4,  3151] loss: 2.297\n",
      "[4,  3201] loss: 2.307\n",
      "[4,  3251] loss: 2.302\n",
      "[4,  3301] loss: 2.305\n",
      "[4,  3351] loss: 2.304\n",
      "[4,  3401] loss: 2.304\n",
      "[4,  3451] loss: 2.301\n",
      "[4,  3501] loss: 2.300\n",
      "[4,  3551] loss: 2.301\n",
      "[4,  3601] loss: 2.300\n",
      "[4,  3651] loss: 2.298\n",
      "[4,  3701] loss: 2.304\n",
      "[4,  3751] loss: 2.300\n",
      "[4,  3801] loss: 2.304\n",
      "[4,  3851] loss: 2.303\n",
      "[4,  3901] loss: 2.305\n",
      "[4,  3951] loss: 2.305\n",
      "[4,  4001] loss: 2.305\n",
      "[4,  4051] loss: 2.309\n",
      "[4,  4101] loss: 2.305\n",
      "[4,  4151] loss: 2.300\n",
      "[4,  4201] loss: 2.302\n",
      "[4,  4251] loss: 2.299\n",
      "[4,  4301] loss: 2.304\n",
      "[4,  4351] loss: 2.303\n",
      "[4,  4401] loss: 2.304\n",
      "[4,  4451] loss: 2.306\n",
      "[4,  4501] loss: 2.300\n",
      "[4,  4551] loss: 2.304\n",
      "[4,  4601] loss: 2.297\n",
      "[4,  4651] loss: 2.305\n",
      "[4,  4701] loss: 2.303\n",
      "[4,  4751] loss: 2.303\n",
      "[4,  4801] loss: 2.305\n",
      "[4,  4851] loss: 2.307\n",
      "[4,  4901] loss: 2.303\n",
      "[4,  4951] loss: 2.303\n",
      "[4,  5001] loss: 2.300\n",
      "[4,  5051] loss: 2.302\n",
      "[4,  5101] loss: 2.305\n",
      "[4,  5151] loss: 2.305\n",
      "[4,  5201] loss: 2.304\n",
      "[4,  5251] loss: 2.303\n",
      "[4,  5301] loss: 2.303\n",
      "[4,  5351] loss: 2.305\n",
      "[4,  5401] loss: 2.304\n",
      "[4,  5451] loss: 2.297\n",
      "[4,  5501] loss: 2.301\n",
      "[4,  5551] loss: 2.308\n",
      "[4,  5601] loss: 2.299\n",
      "[4,  5651] loss: 2.309\n",
      "[4,  5701] loss: 2.300\n",
      "[4,  5751] loss: 2.307\n",
      "[4,  5801] loss: 2.306\n",
      "[4,  5851] loss: 2.304\n",
      "[4,  5901] loss: 2.304\n",
      "[4,  5951] loss: 2.300\n",
      "[4,  6001] loss: 2.305\n",
      "[4,  6051] loss: 2.306\n",
      "[4,  6101] loss: 2.304\n",
      "[4,  6151] loss: 2.306\n",
      "[4,  6201] loss: 2.310\n",
      "[4,  6251] loss: 2.296\n",
      "[4,  6301] loss: 2.302\n",
      "[4,  6351] loss: 2.308\n",
      "[4,  6401] loss: 2.297\n",
      "[4,  6451] loss: 2.304\n",
      "[4,  6501] loss: 2.298\n",
      "[4,  6551] loss: 2.304\n",
      "[4,  6601] loss: 2.306\n",
      "[4,  6651] loss: 2.304\n",
      "[4,  6701] loss: 2.309\n",
      "[4,  6751] loss: 2.305\n",
      "[4,  6801] loss: 2.302\n",
      "[4,  6851] loss: 2.302\n",
      "[4,  6901] loss: 2.299\n",
      "[4,  6951] loss: 2.298\n",
      "[4,  7001] loss: 2.301\n",
      "[4,  7051] loss: 2.307\n",
      "[4,  7101] loss: 2.307\n",
      "[4,  7151] loss: 2.303\n",
      "[4,  7201] loss: 2.307\n",
      "[4,  7251] loss: 2.302\n",
      "[4,  7301] loss: 2.304\n",
      "[4,  7351] loss: 2.308\n",
      "[4,  7401] loss: 2.307\n",
      "[4,  7451] loss: 2.308\n",
      "[4,  7501] loss: 2.302\n",
      "[4,  7551] loss: 2.310\n",
      "[4,  7601] loss: 2.303\n",
      "[4,  7651] loss: 2.302\n",
      "[4,  7701] loss: 2.307\n",
      "[4,  7751] loss: 2.306\n",
      "[4,  7801] loss: 2.304\n",
      "[4,  7851] loss: 2.302\n",
      "[4,  7901] loss: 2.297\n",
      "[4,  7951] loss: 2.307\n",
      "[4,  8001] loss: 2.307\n",
      "[4,  8051] loss: 2.307\n",
      "[4,  8101] loss: 2.308\n",
      "[4,  8151] loss: 2.309\n",
      "[4,  8201] loss: 2.305\n",
      "[4,  8251] loss: 2.304\n",
      "[4,  8301] loss: 2.304\n",
      "[4,  8351] loss: 2.299\n",
      "[4,  8401] loss: 2.303\n",
      "[4,  8451] loss: 2.302\n",
      "[4,  8501] loss: 2.302\n",
      "[4,  8551] loss: 2.303\n",
      "[4,  8601] loss: 2.302\n",
      "[4,  8651] loss: 2.305\n",
      "[4,  8701] loss: 2.309\n",
      "[4,  8751] loss: 2.298\n",
      "[4,  8801] loss: 2.304\n",
      "[4,  8851] loss: 2.304\n",
      "[4,  8901] loss: 2.307\n",
      "[4,  8951] loss: 2.301\n",
      "[4,  9001] loss: 2.303\n",
      "[4,  9051] loss: 2.303\n",
      "[4,  9101] loss: 2.306\n",
      "[4,  9151] loss: 2.302\n",
      "[4,  9201] loss: 2.302\n",
      "[4,  9251] loss: 2.307\n",
      "[4,  9301] loss: 2.302\n",
      "[4,  9351] loss: 2.302\n",
      "[4,  9401] loss: 2.306\n",
      "[4,  9451] loss: 2.303\n",
      "[4,  9501] loss: 2.300\n",
      "[4,  9551] loss: 2.306\n",
      "[4,  9601] loss: 2.302\n",
      "[4,  9651] loss: 2.304\n",
      "[4,  9701] loss: 2.305\n",
      "[4,  9751] loss: 2.305\n",
      "[4,  9801] loss: 2.305\n",
      "[4,  9851] loss: 2.300\n",
      "[4,  9901] loss: 2.308\n",
      "[4,  9951] loss: 2.305\n",
      "[4, 10001] loss: 2.303\n",
      "[4, 10051] loss: 2.305\n",
      "[4, 10101] loss: 2.309\n",
      "[4, 10151] loss: 2.305\n",
      "[4, 10201] loss: 2.301\n",
      "[4, 10251] loss: 2.302\n",
      "[4, 10301] loss: 2.303\n",
      "[4, 10351] loss: 2.300\n",
      "[4, 10401] loss: 2.301\n",
      "[4, 10451] loss: 2.306\n",
      "[4, 10501] loss: 2.303\n",
      "[4, 10551] loss: 2.300\n",
      "[4, 10601] loss: 2.303\n",
      "[4, 10651] loss: 2.306\n",
      "[4, 10701] loss: 2.309\n",
      "[4, 10751] loss: 2.304\n",
      "[4, 10801] loss: 2.300\n",
      "[4, 10851] loss: 2.307\n",
      "[4, 10901] loss: 2.309\n",
      "[4, 10951] loss: 2.302\n",
      "[4, 11001] loss: 2.304\n",
      "[4, 11051] loss: 2.303\n",
      "[4, 11101] loss: 2.298\n",
      "[4, 11151] loss: 2.301\n",
      "[4, 11201] loss: 2.303\n",
      "[4, 11251] loss: 2.301\n",
      "[4, 11301] loss: 2.307\n",
      "[4, 11351] loss: 2.302\n",
      "[4, 11401] loss: 2.302\n",
      "[4, 11451] loss: 2.300\n",
      "[4, 11501] loss: 2.295\n",
      "[4, 11551] loss: 2.308\n",
      "[4, 11601] loss: 2.300\n",
      "[4, 11651] loss: 2.302\n",
      "[4, 11701] loss: 2.309\n",
      "[4, 11751] loss: 2.303\n",
      "[4, 11801] loss: 2.304\n",
      "[4, 11851] loss: 2.298\n",
      "[4, 11901] loss: 2.300\n",
      "[4, 11951] loss: 2.304\n",
      "[4, 12001] loss: 2.305\n",
      "[4, 12051] loss: 2.304\n",
      "[4, 12101] loss: 2.306\n",
      "[4, 12151] loss: 2.303\n",
      "[4, 12201] loss: 2.298\n",
      "[4, 12251] loss: 2.307\n",
      "[4, 12301] loss: 2.303\n",
      "[4, 12351] loss: 2.300\n",
      "[4, 12401] loss: 2.307\n",
      "[4, 12451] loss: 2.305\n",
      "[5,     1] loss: 0.046\n",
      "[5,    51] loss: 2.304\n",
      "[5,   101] loss: 2.307\n",
      "[5,   151] loss: 2.305\n",
      "[5,   201] loss: 2.296\n",
      "[5,   251] loss: 2.303\n",
      "[5,   301] loss: 2.304\n",
      "[5,   351] loss: 2.302\n",
      "[5,   401] loss: 2.304\n",
      "[5,   451] loss: 2.302\n",
      "[5,   501] loss: 2.300\n",
      "[5,   551] loss: 2.298\n",
      "[5,   601] loss: 2.303\n",
      "[5,   651] loss: 2.305\n",
      "[5,   701] loss: 2.309\n",
      "[5,   751] loss: 2.309\n",
      "[5,   801] loss: 2.301\n",
      "[5,   851] loss: 2.304\n",
      "[5,   901] loss: 2.301\n",
      "[5,   951] loss: 2.300\n",
      "[5,  1001] loss: 2.309\n",
      "[5,  1051] loss: 2.301\n",
      "[5,  1101] loss: 2.308\n",
      "[5,  1151] loss: 2.302\n",
      "[5,  1201] loss: 2.302\n",
      "[5,  1251] loss: 2.306\n",
      "[5,  1301] loss: 2.299\n",
      "[5,  1351] loss: 2.304\n",
      "[5,  1401] loss: 2.309\n",
      "[5,  1451] loss: 2.300\n",
      "[5,  1501] loss: 2.309\n",
      "[5,  1551] loss: 2.299\n",
      "[5,  1601] loss: 2.304\n",
      "[5,  1651] loss: 2.305\n",
      "[5,  1701] loss: 2.300\n",
      "[5,  1751] loss: 2.303\n",
      "[5,  1801] loss: 2.306\n",
      "[5,  1851] loss: 2.304\n",
      "[5,  1901] loss: 2.304\n",
      "[5,  1951] loss: 2.304\n",
      "[5,  2001] loss: 2.306\n",
      "[5,  2051] loss: 2.304\n",
      "[5,  2101] loss: 2.301\n",
      "[5,  2151] loss: 2.302\n",
      "[5,  2201] loss: 2.303\n",
      "[5,  2251] loss: 2.296\n",
      "[5,  2301] loss: 2.306\n",
      "[5,  2351] loss: 2.304\n",
      "[5,  2401] loss: 2.305\n",
      "[5,  2451] loss: 2.304\n",
      "[5,  2501] loss: 2.302\n",
      "[5,  2551] loss: 2.309\n",
      "[5,  2601] loss: 2.308\n",
      "[5,  2651] loss: 2.306\n",
      "[5,  2701] loss: 2.302\n",
      "[5,  2751] loss: 2.300\n",
      "[5,  2801] loss: 2.305\n",
      "[5,  2851] loss: 2.306\n",
      "[5,  2901] loss: 2.314\n",
      "[5,  2951] loss: 2.307\n",
      "[5,  3001] loss: 2.301\n",
      "[5,  3051] loss: 2.302\n",
      "[5,  3101] loss: 2.303\n",
      "[5,  3151] loss: 2.301\n",
      "[5,  3201] loss: 2.304\n",
      "[5,  3251] loss: 2.306\n",
      "[5,  3301] loss: 2.303\n",
      "[5,  3351] loss: 2.299\n",
      "[5,  3401] loss: 2.303\n",
      "[5,  3451] loss: 2.307\n",
      "[5,  3501] loss: 2.301\n",
      "[5,  3551] loss: 2.299\n",
      "[5,  3601] loss: 2.301\n",
      "[5,  3651] loss: 2.308\n",
      "[5,  3701] loss: 2.308\n",
      "[5,  3751] loss: 2.298\n",
      "[5,  3801] loss: 2.298\n",
      "[5,  3851] loss: 2.309\n",
      "[5,  3901] loss: 2.299\n",
      "[5,  3951] loss: 2.305\n",
      "[5,  4001] loss: 2.303\n",
      "[5,  4051] loss: 2.304\n",
      "[5,  4101] loss: 2.306\n",
      "[5,  4151] loss: 2.306\n",
      "[5,  4201] loss: 2.301\n",
      "[5,  4251] loss: 2.299\n",
      "[5,  4301] loss: 2.303\n",
      "[5,  4351] loss: 2.305\n",
      "[5,  4401] loss: 2.303\n",
      "[5,  4451] loss: 2.300\n",
      "[5,  4501] loss: 2.305\n",
      "[5,  4551] loss: 2.307\n",
      "[5,  4601] loss: 2.303\n",
      "[5,  4651] loss: 2.305\n",
      "[5,  4701] loss: 2.301\n",
      "[5,  4751] loss: 2.304\n",
      "[5,  4801] loss: 2.303\n",
      "[5,  4851] loss: 2.303\n",
      "[5,  4901] loss: 2.297\n",
      "[5,  4951] loss: 2.305\n",
      "[5,  5001] loss: 2.308\n",
      "[5,  5051] loss: 2.304\n",
      "[5,  5101] loss: 2.300\n",
      "[5,  5151] loss: 2.305\n",
      "[5,  5201] loss: 2.306\n",
      "[5,  5251] loss: 2.299\n",
      "[5,  5301] loss: 2.308\n",
      "[5,  5351] loss: 2.303\n",
      "[5,  5401] loss: 2.305\n",
      "[5,  5451] loss: 2.307\n",
      "[5,  5501] loss: 2.303\n",
      "[5,  5551] loss: 2.305\n",
      "[5,  5601] loss: 2.307\n",
      "[5,  5651] loss: 2.306\n",
      "[5,  5701] loss: 2.304\n",
      "[5,  5751] loss: 2.300\n",
      "[5,  5801] loss: 2.302\n",
      "[5,  5851] loss: 2.304\n",
      "[5,  5901] loss: 2.303\n",
      "[5,  5951] loss: 2.304\n",
      "[5,  6001] loss: 2.307\n",
      "[5,  6051] loss: 2.292\n",
      "[5,  6101] loss: 2.302\n",
      "[5,  6151] loss: 2.301\n",
      "[5,  6201] loss: 2.306\n",
      "[5,  6251] loss: 2.303\n",
      "[5,  6301] loss: 2.301\n",
      "[5,  6351] loss: 2.309\n",
      "[5,  6401] loss: 2.305\n",
      "[5,  6451] loss: 2.306\n",
      "[5,  6501] loss: 2.305\n",
      "[5,  6551] loss: 2.304\n",
      "[5,  6601] loss: 2.305\n",
      "[5,  6651] loss: 2.299\n",
      "[5,  6701] loss: 2.305\n",
      "[5,  6751] loss: 2.303\n",
      "[5,  6801] loss: 2.306\n",
      "[5,  6851] loss: 2.308\n",
      "[5,  6901] loss: 2.304\n",
      "[5,  6951] loss: 2.301\n",
      "[5,  7001] loss: 2.303\n",
      "[5,  7051] loss: 2.303\n",
      "[5,  7101] loss: 2.302\n",
      "[5,  7151] loss: 2.306\n",
      "[5,  7201] loss: 2.304\n",
      "[5,  7251] loss: 2.304\n",
      "[5,  7301] loss: 2.308\n",
      "[5,  7351] loss: 2.302\n",
      "[5,  7401] loss: 2.300\n",
      "[5,  7451] loss: 2.307\n",
      "[5,  7501] loss: 2.306\n",
      "[5,  7551] loss: 2.300\n",
      "[5,  7601] loss: 2.305\n",
      "[5,  7651] loss: 2.306\n",
      "[5,  7701] loss: 2.308\n",
      "[5,  7751] loss: 2.305\n",
      "[5,  7801] loss: 2.301\n",
      "[5,  7851] loss: 2.305\n",
      "[5,  7901] loss: 2.304\n",
      "[5,  7951] loss: 2.302\n",
      "[5,  8001] loss: 2.304\n",
      "[5,  8051] loss: 2.309\n",
      "[5,  8101] loss: 2.309\n",
      "[5,  8151] loss: 2.301\n",
      "[5,  8201] loss: 2.301\n",
      "[5,  8251] loss: 2.305\n",
      "[5,  8301] loss: 2.308\n",
      "[5,  8351] loss: 2.301\n",
      "[5,  8401] loss: 2.303\n",
      "[5,  8451] loss: 2.302\n",
      "[5,  8501] loss: 2.302\n",
      "[5,  8551] loss: 2.302\n",
      "[5,  8601] loss: 2.307\n",
      "[5,  8651] loss: 2.303\n",
      "[5,  8701] loss: 2.304\n",
      "[5,  8751] loss: 2.306\n",
      "[5,  8801] loss: 2.305\n",
      "[5,  8851] loss: 2.301\n",
      "[5,  8901] loss: 2.300\n",
      "[5,  8951] loss: 2.307\n",
      "[5,  9001] loss: 2.300\n",
      "[5,  9051] loss: 2.301\n",
      "[5,  9101] loss: 2.300\n",
      "[5,  9151] loss: 2.306\n",
      "[5,  9201] loss: 2.303\n",
      "[5,  9251] loss: 2.305\n",
      "[5,  9301] loss: 2.305\n",
      "[5,  9351] loss: 2.302\n",
      "[5,  9401] loss: 2.301\n",
      "[5,  9451] loss: 2.301\n",
      "[5,  9501] loss: 2.307\n",
      "[5,  9551] loss: 2.308\n",
      "[5,  9601] loss: 2.306\n",
      "[5,  9651] loss: 2.302\n",
      "[5,  9701] loss: 2.305\n",
      "[5,  9751] loss: 2.304\n",
      "[5,  9801] loss: 2.305\n",
      "[5,  9851] loss: 2.303\n",
      "[5,  9901] loss: 2.310\n",
      "[5,  9951] loss: 2.299\n",
      "[5, 10001] loss: 2.302\n",
      "[5, 10051] loss: 2.304\n",
      "[5, 10101] loss: 2.299\n",
      "[5, 10151] loss: 2.308\n",
      "[5, 10201] loss: 2.306\n",
      "[5, 10251] loss: 2.304\n",
      "[5, 10301] loss: 2.299\n",
      "[5, 10351] loss: 2.303\n",
      "[5, 10401] loss: 2.307\n",
      "[5, 10451] loss: 2.300\n",
      "[5, 10501] loss: 2.302\n",
      "[5, 10551] loss: 2.300\n",
      "[5, 10601] loss: 2.308\n",
      "[5, 10651] loss: 2.302\n",
      "[5, 10701] loss: 2.305\n",
      "[5, 10751] loss: 2.309\n",
      "[5, 10801] loss: 2.301\n",
      "[5, 10851] loss: 2.301\n",
      "[5, 10901] loss: 2.309\n",
      "[5, 10951] loss: 2.299\n",
      "[5, 11001] loss: 2.302\n",
      "[5, 11051] loss: 2.308\n",
      "[5, 11101] loss: 2.301\n",
      "[5, 11151] loss: 2.308\n",
      "[5, 11201] loss: 2.304\n",
      "[5, 11251] loss: 2.302\n",
      "[5, 11301] loss: 2.301\n",
      "[5, 11351] loss: 2.307\n",
      "[5, 11401] loss: 2.301\n",
      "[5, 11451] loss: 2.297\n",
      "[5, 11501] loss: 2.307\n",
      "[5, 11551] loss: 2.307\n",
      "[5, 11601] loss: 2.301\n",
      "[5, 11651] loss: 2.302\n",
      "[5, 11701] loss: 2.295\n",
      "[5, 11751] loss: 2.300\n",
      "[5, 11801] loss: 2.300\n",
      "[5, 11851] loss: 2.304\n",
      "[5, 11901] loss: 2.302\n",
      "[5, 11951] loss: 2.306\n",
      "[5, 12001] loss: 2.304\n",
      "[5, 12051] loss: 2.312\n",
      "[5, 12101] loss: 2.306\n",
      "[5, 12151] loss: 2.302\n",
      "[5, 12201] loss: 2.303\n",
      "[5, 12251] loss: 2.304\n",
      "[5, 12301] loss: 2.301\n",
      "[5, 12351] loss: 2.310\n",
      "[5, 12401] loss: 2.300\n",
      "[5, 12451] loss: 2.302\n",
      "[6,     1] loss: 0.046\n",
      "[6,    51] loss: 2.308\n",
      "[6,   101] loss: 2.302\n",
      "[6,   151] loss: 2.306\n",
      "[6,   201] loss: 2.306\n",
      "[6,   251] loss: 2.305\n",
      "[6,   301] loss: 2.304\n",
      "[6,   351] loss: 2.299\n",
      "[6,   401] loss: 2.304\n",
      "[6,   451] loss: 2.308\n",
      "[6,   501] loss: 2.305\n",
      "[6,   551] loss: 2.307\n",
      "[6,   601] loss: 2.313\n",
      "[6,   651] loss: 2.305\n",
      "[6,   701] loss: 2.308\n",
      "[6,   751] loss: 2.309\n",
      "[6,   801] loss: 2.307\n",
      "[6,   851] loss: 2.302\n",
      "[6,   901] loss: 2.302\n",
      "[6,   951] loss: 2.301\n",
      "[6,  1001] loss: 2.302\n",
      "[6,  1051] loss: 2.304\n",
      "[6,  1101] loss: 2.308\n",
      "[6,  1151] loss: 2.303\n",
      "[6,  1201] loss: 2.307\n",
      "[6,  1251] loss: 2.300\n",
      "[6,  1301] loss: 2.302\n",
      "[6,  1351] loss: 2.301\n",
      "[6,  1401] loss: 2.301\n",
      "[6,  1451] loss: 2.304\n",
      "[6,  1501] loss: 2.300\n",
      "[6,  1551] loss: 2.310\n",
      "[6,  1601] loss: 2.297\n",
      "[6,  1651] loss: 2.299\n",
      "[6,  1701] loss: 2.306\n",
      "[6,  1751] loss: 2.304\n",
      "[6,  1801] loss: 2.302\n",
      "[6,  1851] loss: 2.296\n",
      "[6,  1901] loss: 2.299\n",
      "[6,  1951] loss: 2.305\n",
      "[6,  2001] loss: 2.302\n",
      "[6,  2051] loss: 2.301\n",
      "[6,  2101] loss: 2.309\n",
      "[6,  2151] loss: 2.306\n",
      "[6,  2201] loss: 2.300\n",
      "[6,  2251] loss: 2.309\n",
      "[6,  2301] loss: 2.307\n",
      "[6,  2351] loss: 2.309\n",
      "[6,  2401] loss: 2.295\n",
      "[6,  2451] loss: 2.299\n",
      "[6,  2501] loss: 2.303\n",
      "[6,  2551] loss: 2.299\n",
      "[6,  2601] loss: 2.303\n",
      "[6,  2651] loss: 2.308\n",
      "[6,  2701] loss: 2.310\n",
      "[6,  2751] loss: 2.306\n",
      "[6,  2801] loss: 2.303\n",
      "[6,  2851] loss: 2.311\n",
      "[6,  2901] loss: 2.308\n",
      "[6,  2951] loss: 2.313\n",
      "[6,  3001] loss: 2.298\n",
      "[6,  3051] loss: 2.308\n",
      "[6,  3101] loss: 2.304\n",
      "[6,  3151] loss: 2.303\n",
      "[6,  3201] loss: 2.300\n",
      "[6,  3251] loss: 2.310\n",
      "[6,  3301] loss: 2.302\n",
      "[6,  3351] loss: 2.308\n",
      "[6,  3401] loss: 2.308\n",
      "[6,  3451] loss: 2.305\n",
      "[6,  3501] loss: 2.301\n",
      "[6,  3551] loss: 2.300\n",
      "[6,  3601] loss: 2.305\n",
      "[6,  3651] loss: 2.302\n",
      "[6,  3701] loss: 2.303\n",
      "[6,  3751] loss: 2.309\n",
      "[6,  3801] loss: 2.298\n",
      "[6,  3851] loss: 2.302\n",
      "[6,  3901] loss: 2.302\n",
      "[6,  3951] loss: 2.303\n",
      "[6,  4001] loss: 2.301\n",
      "[6,  4051] loss: 2.302\n",
      "[6,  4101] loss: 2.304\n",
      "[6,  4151] loss: 2.302\n",
      "[6,  4201] loss: 2.307\n",
      "[6,  4251] loss: 2.308\n",
      "[6,  4301] loss: 2.304\n",
      "[6,  4351] loss: 2.300\n",
      "[6,  4401] loss: 2.299\n",
      "[6,  4451] loss: 2.305\n",
      "[6,  4501] loss: 2.295\n",
      "[6,  4551] loss: 2.301\n",
      "[6,  4601] loss: 2.301\n",
      "[6,  4651] loss: 2.296\n",
      "[6,  4701] loss: 2.299\n",
      "[6,  4751] loss: 2.300\n",
      "[6,  4801] loss: 2.305\n",
      "[6,  4851] loss: 2.303\n",
      "[6,  4901] loss: 2.306\n",
      "[6,  4951] loss: 2.308\n",
      "[6,  5001] loss: 2.301\n",
      "[6,  5051] loss: 2.309\n",
      "[6,  5101] loss: 2.307\n",
      "[6,  5151] loss: 2.302\n",
      "[6,  5201] loss: 2.306\n",
      "[6,  5251] loss: 2.300\n",
      "[6,  5301] loss: 2.300\n",
      "[6,  5351] loss: 2.298\n",
      "[6,  5401] loss: 2.297\n",
      "[6,  5451] loss: 2.306\n",
      "[6,  5501] loss: 2.304\n",
      "[6,  5551] loss: 2.297\n",
      "[6,  5601] loss: 2.304\n",
      "[6,  5651] loss: 2.302\n",
      "[6,  5701] loss: 2.308\n",
      "[6,  5751] loss: 2.300\n",
      "[6,  5801] loss: 2.304\n",
      "[6,  5851] loss: 2.304\n",
      "[6,  5901] loss: 2.299\n",
      "[6,  5951] loss: 2.299\n",
      "[6,  6001] loss: 2.307\n",
      "[6,  6051] loss: 2.300\n",
      "[6,  6101] loss: 2.303\n",
      "[6,  6151] loss: 2.308\n",
      "[6,  6201] loss: 2.306\n",
      "[6,  6251] loss: 2.308\n",
      "[6,  6301] loss: 2.300\n",
      "[6,  6351] loss: 2.305\n",
      "[6,  6401] loss: 2.309\n",
      "[6,  6451] loss: 2.307\n",
      "[6,  6501] loss: 2.303\n",
      "[6,  6551] loss: 2.297\n",
      "[6,  6601] loss: 2.303\n",
      "[6,  6651] loss: 2.301\n",
      "[6,  6701] loss: 2.306\n",
      "[6,  6751] loss: 2.305\n",
      "[6,  6801] loss: 2.306\n",
      "[6,  6851] loss: 2.304\n",
      "[6,  6901] loss: 2.308\n",
      "[6,  6951] loss: 2.300\n",
      "[6,  7001] loss: 2.306\n",
      "[6,  7051] loss: 2.303\n",
      "[6,  7101] loss: 2.305\n",
      "[6,  7151] loss: 2.297\n",
      "[6,  7201] loss: 2.302\n",
      "[6,  7251] loss: 2.307\n",
      "[6,  7301] loss: 2.305\n",
      "[6,  7351] loss: 2.303\n",
      "[6,  7401] loss: 2.307\n",
      "[6,  7451] loss: 2.303\n",
      "[6,  7501] loss: 2.305\n",
      "[6,  7551] loss: 2.302\n",
      "[6,  7601] loss: 2.304\n",
      "[6,  7651] loss: 2.308\n",
      "[6,  7701] loss: 2.303\n",
      "[6,  7751] loss: 2.306\n",
      "[6,  7801] loss: 2.304\n",
      "[6,  7851] loss: 2.301\n",
      "[6,  7901] loss: 2.299\n",
      "[6,  7951] loss: 2.297\n",
      "[6,  8001] loss: 2.304\n",
      "[6,  8051] loss: 2.307\n",
      "[6,  8101] loss: 2.309\n",
      "[6,  8151] loss: 2.310\n",
      "[6,  8201] loss: 2.303\n",
      "[6,  8251] loss: 2.302\n",
      "[6,  8301] loss: 2.306\n",
      "[6,  8351] loss: 2.299\n",
      "[6,  8401] loss: 2.305\n",
      "[6,  8451] loss: 2.301\n",
      "[6,  8501] loss: 2.300\n",
      "[6,  8551] loss: 2.300\n",
      "[6,  8601] loss: 2.306\n",
      "[6,  8651] loss: 2.303\n",
      "[6,  8701] loss: 2.300\n",
      "[6,  8751] loss: 2.306\n",
      "[6,  8801] loss: 2.307\n",
      "[6,  8851] loss: 2.307\n",
      "[6,  8901] loss: 2.299\n",
      "[6,  8951] loss: 2.306\n",
      "[6,  9001] loss: 2.303\n",
      "[6,  9051] loss: 2.306\n",
      "[6,  9101] loss: 2.299\n",
      "[6,  9151] loss: 2.308\n",
      "[6,  9201] loss: 2.304\n",
      "[6,  9251] loss: 2.300\n",
      "[6,  9301] loss: 2.307\n",
      "[6,  9351] loss: 2.303\n",
      "[6,  9401] loss: 2.300\n",
      "[6,  9451] loss: 2.307\n",
      "[6,  9501] loss: 2.299\n",
      "[6,  9551] loss: 2.299\n",
      "[6,  9601] loss: 2.300\n",
      "[6,  9651] loss: 2.304\n",
      "[6,  9701] loss: 2.300\n",
      "[6,  9751] loss: 2.298\n",
      "[6,  9801] loss: 2.305\n",
      "[6,  9851] loss: 2.301\n",
      "[6,  9901] loss: 2.306\n",
      "[6,  9951] loss: 2.303\n",
      "[6, 10001] loss: 2.306\n",
      "[6, 10051] loss: 2.300\n",
      "[6, 10101] loss: 2.305\n",
      "[6, 10151] loss: 2.306\n",
      "[6, 10201] loss: 2.306\n",
      "[6, 10251] loss: 2.302\n",
      "[6, 10301] loss: 2.300\n",
      "[6, 10351] loss: 2.306\n",
      "[6, 10401] loss: 2.307\n",
      "[6, 10451] loss: 2.302\n",
      "[6, 10501] loss: 2.308\n",
      "[6, 10551] loss: 2.303\n",
      "[6, 10601] loss: 2.304\n",
      "[6, 10651] loss: 2.300\n",
      "[6, 10701] loss: 2.302\n",
      "[6, 10751] loss: 2.300\n",
      "[6, 10801] loss: 2.308\n",
      "[6, 10851] loss: 2.309\n",
      "[6, 10901] loss: 2.307\n",
      "[6, 10951] loss: 2.304\n",
      "[6, 11001] loss: 2.296\n",
      "[6, 11051] loss: 2.301\n",
      "[6, 11101] loss: 2.305\n",
      "[6, 11151] loss: 2.297\n",
      "[6, 11201] loss: 2.306\n",
      "[6, 11251] loss: 2.303\n",
      "[6, 11301] loss: 2.303\n",
      "[6, 11351] loss: 2.302\n",
      "[6, 11401] loss: 2.308\n",
      "[6, 11451] loss: 2.312\n",
      "[6, 11501] loss: 2.306\n",
      "[6, 11551] loss: 2.299\n",
      "[6, 11601] loss: 2.300\n",
      "[6, 11651] loss: 2.309\n",
      "[6, 11701] loss: 2.303\n",
      "[6, 11751] loss: 2.304\n",
      "[6, 11801] loss: 2.307\n",
      "[6, 11851] loss: 2.301\n",
      "[6, 11901] loss: 2.304\n",
      "[6, 11951] loss: 2.300\n",
      "[6, 12001] loss: 2.299\n",
      "[6, 12051] loss: 2.308\n",
      "[6, 12101] loss: 2.303\n",
      "[6, 12151] loss: 2.299\n",
      "[6, 12201] loss: 2.307\n",
      "[6, 12251] loss: 2.307\n",
      "[6, 12301] loss: 2.302\n",
      "[6, 12351] loss: 2.302\n",
      "[6, 12401] loss: 2.307\n",
      "[6, 12451] loss: 2.307\n",
      "[7,     1] loss: 0.046\n",
      "[7,    51] loss: 2.300\n",
      "[7,   101] loss: 2.305\n",
      "[7,   151] loss: 2.301\n",
      "[7,   201] loss: 2.299\n",
      "[7,   251] loss: 2.305\n",
      "[7,   301] loss: 2.302\n",
      "[7,   351] loss: 2.299\n",
      "[7,   401] loss: 2.301\n",
      "[7,   451] loss: 2.300\n",
      "[7,   501] loss: 2.308\n",
      "[7,   551] loss: 2.306\n",
      "[7,   601] loss: 2.303\n",
      "[7,   651] loss: 2.298\n",
      "[7,   701] loss: 2.303\n",
      "[7,   751] loss: 2.302\n",
      "[7,   801] loss: 2.302\n",
      "[7,   851] loss: 2.308\n",
      "[7,   901] loss: 2.301\n",
      "[7,   951] loss: 2.301\n",
      "[7,  1001] loss: 2.306\n",
      "[7,  1051] loss: 2.305\n",
      "[7,  1101] loss: 2.303\n",
      "[7,  1151] loss: 2.304\n",
      "[7,  1201] loss: 2.301\n",
      "[7,  1251] loss: 2.300\n",
      "[7,  1301] loss: 2.305\n",
      "[7,  1351] loss: 2.304\n",
      "[7,  1401] loss: 2.305\n",
      "[7,  1451] loss: 2.307\n",
      "[7,  1501] loss: 2.304\n",
      "[7,  1551] loss: 2.304\n",
      "[7,  1601] loss: 2.306\n",
      "[7,  1651] loss: 2.298\n",
      "[7,  1701] loss: 2.304\n",
      "[7,  1751] loss: 2.310\n",
      "[7,  1801] loss: 2.302\n",
      "[7,  1851] loss: 2.303\n",
      "[7,  1901] loss: 2.304\n",
      "[7,  1951] loss: 2.294\n",
      "[7,  2001] loss: 2.307\n",
      "[7,  2051] loss: 2.301\n",
      "[7,  2101] loss: 2.298\n",
      "[7,  2151] loss: 2.304\n",
      "[7,  2201] loss: 2.297\n",
      "[7,  2251] loss: 2.303\n",
      "[7,  2301] loss: 2.307\n",
      "[7,  2351] loss: 2.301\n",
      "[7,  2401] loss: 2.310\n",
      "[7,  2451] loss: 2.305\n",
      "[7,  2501] loss: 2.305\n",
      "[7,  2551] loss: 2.312\n",
      "[7,  2601] loss: 2.299\n",
      "[7,  2651] loss: 2.301\n",
      "[7,  2701] loss: 2.305\n",
      "[7,  2751] loss: 2.300\n",
      "[7,  2801] loss: 2.306\n",
      "[7,  2851] loss: 2.306\n",
      "[7,  2901] loss: 2.302\n",
      "[7,  2951] loss: 2.307\n",
      "[7,  3001] loss: 2.305\n",
      "[7,  3051] loss: 2.310\n",
      "[7,  3101] loss: 2.311\n",
      "[7,  3151] loss: 2.300\n",
      "[7,  3201] loss: 2.305\n",
      "[7,  3251] loss: 2.301\n",
      "[7,  3301] loss: 2.300\n",
      "[7,  3351] loss: 2.301\n",
      "[7,  3401] loss: 2.306\n",
      "[7,  3451] loss: 2.301\n",
      "[7,  3501] loss: 2.304\n",
      "[7,  3551] loss: 2.307\n",
      "[7,  3601] loss: 2.302\n",
      "[7,  3651] loss: 2.301\n",
      "[7,  3701] loss: 2.308\n",
      "[7,  3751] loss: 2.300\n",
      "[7,  3801] loss: 2.303\n",
      "[7,  3851] loss: 2.303\n",
      "[7,  3901] loss: 2.306\n",
      "[7,  3951] loss: 2.300\n",
      "[7,  4001] loss: 2.307\n",
      "[7,  4051] loss: 2.299\n",
      "[7,  4101] loss: 2.301\n",
      "[7,  4151] loss: 2.308\n",
      "[7,  4201] loss: 2.303\n",
      "[7,  4251] loss: 2.303\n",
      "[7,  4301] loss: 2.298\n",
      "[7,  4351] loss: 2.303\n",
      "[7,  4401] loss: 2.308\n",
      "[7,  4451] loss: 2.304\n",
      "[7,  4501] loss: 2.308\n",
      "[7,  4551] loss: 2.305\n",
      "[7,  4601] loss: 2.304\n",
      "[7,  4651] loss: 2.303\n",
      "[7,  4701] loss: 2.302\n",
      "[7,  4751] loss: 2.302\n",
      "[7,  4801] loss: 2.304\n",
      "[7,  4851] loss: 2.306\n",
      "[7,  4901] loss: 2.307\n",
      "[7,  4951] loss: 2.305\n",
      "[7,  5001] loss: 2.301\n",
      "[7,  5051] loss: 2.309\n",
      "[7,  5101] loss: 2.303\n",
      "[7,  5151] loss: 2.302\n",
      "[7,  5201] loss: 2.298\n",
      "[7,  5251] loss: 2.303\n",
      "[7,  5301] loss: 2.301\n",
      "[7,  5351] loss: 2.302\n",
      "[7,  5401] loss: 2.295\n",
      "[7,  5451] loss: 2.304\n",
      "[7,  5501] loss: 2.303\n",
      "[7,  5551] loss: 2.307\n",
      "[7,  5601] loss: 2.306\n",
      "[7,  5651] loss: 2.302\n",
      "[7,  5701] loss: 2.302\n",
      "[7,  5751] loss: 2.308\n",
      "[7,  5801] loss: 2.303\n",
      "[7,  5851] loss: 2.306\n",
      "[7,  5901] loss: 2.303\n",
      "[7,  5951] loss: 2.302\n",
      "[7,  6001] loss: 2.302\n",
      "[7,  6051] loss: 2.306\n",
      "[7,  6101] loss: 2.305\n",
      "[7,  6151] loss: 2.298\n",
      "[7,  6201] loss: 2.308\n",
      "[7,  6251] loss: 2.305\n",
      "[7,  6301] loss: 2.305\n",
      "[7,  6351] loss: 2.301\n",
      "[7,  6401] loss: 2.304\n",
      "[7,  6451] loss: 2.307\n",
      "[7,  6501] loss: 2.304\n",
      "[7,  6551] loss: 2.302\n",
      "[7,  6601] loss: 2.300\n",
      "[7,  6651] loss: 2.301\n",
      "[7,  6701] loss: 2.300\n",
      "[7,  6751] loss: 2.304\n",
      "[7,  6801] loss: 2.308\n",
      "[7,  6851] loss: 2.302\n",
      "[7,  6901] loss: 2.302\n",
      "[7,  6951] loss: 2.305\n",
      "[7,  7001] loss: 2.307\n",
      "[7,  7051] loss: 2.303\n",
      "[7,  7101] loss: 2.303\n",
      "[7,  7151] loss: 2.302\n",
      "[7,  7201] loss: 2.311\n",
      "[7,  7251] loss: 2.305\n",
      "[7,  7301] loss: 2.307\n",
      "[7,  7351] loss: 2.302\n",
      "[7,  7401] loss: 2.306\n",
      "[7,  7451] loss: 2.305\n",
      "[7,  7501] loss: 2.298\n",
      "[7,  7551] loss: 2.297\n",
      "[7,  7601] loss: 2.303\n",
      "[7,  7651] loss: 2.304\n",
      "[7,  7701] loss: 2.307\n",
      "[7,  7751] loss: 2.304\n",
      "[7,  7801] loss: 2.308\n",
      "[7,  7851] loss: 2.305\n",
      "[7,  7901] loss: 2.307\n",
      "[7,  7951] loss: 2.299\n",
      "[7,  8001] loss: 2.305\n",
      "[7,  8051] loss: 2.301\n",
      "[7,  8101] loss: 2.303\n",
      "[7,  8151] loss: 2.304\n",
      "[7,  8201] loss: 2.304\n",
      "[7,  8251] loss: 2.298\n",
      "[7,  8301] loss: 2.302\n",
      "[7,  8351] loss: 2.306\n",
      "[7,  8401] loss: 2.303\n",
      "[7,  8451] loss: 2.303\n",
      "[7,  8501] loss: 2.299\n",
      "[7,  8551] loss: 2.302\n",
      "[7,  8601] loss: 2.304\n",
      "[7,  8651] loss: 2.306\n",
      "[7,  8701] loss: 2.304\n",
      "[7,  8751] loss: 2.307\n",
      "[7,  8801] loss: 2.303\n",
      "[7,  8851] loss: 2.306\n",
      "[7,  8901] loss: 2.302\n",
      "[7,  8951] loss: 2.303\n",
      "[7,  9001] loss: 2.305\n",
      "[7,  9051] loss: 2.308\n",
      "[7,  9101] loss: 2.307\n",
      "[7,  9151] loss: 2.306\n",
      "[7,  9201] loss: 2.304\n",
      "[7,  9251] loss: 2.299\n",
      "[7,  9301] loss: 2.306\n",
      "[7,  9351] loss: 2.304\n",
      "[7,  9401] loss: 2.297\n",
      "[7,  9451] loss: 2.305\n",
      "[7,  9501] loss: 2.304\n",
      "[7,  9551] loss: 2.298\n",
      "[7,  9601] loss: 2.304\n",
      "[7,  9651] loss: 2.302\n",
      "[7,  9701] loss: 2.302\n",
      "[7,  9751] loss: 2.304\n",
      "[7,  9801] loss: 2.308\n",
      "[7,  9851] loss: 2.307\n",
      "[7,  9901] loss: 2.298\n",
      "[7,  9951] loss: 2.303\n",
      "[7, 10001] loss: 2.302\n",
      "[7, 10051] loss: 2.305\n",
      "[7, 10101] loss: 2.298\n",
      "[7, 10151] loss: 2.307\n",
      "[7, 10201] loss: 2.306\n",
      "[7, 10251] loss: 2.304\n",
      "[7, 10301] loss: 2.305\n",
      "[7, 10351] loss: 2.303\n",
      "[7, 10401] loss: 2.302\n",
      "[7, 10451] loss: 2.303\n",
      "[7, 10501] loss: 2.308\n",
      "[7, 10551] loss: 2.304\n",
      "[7, 10601] loss: 2.306\n",
      "[7, 10651] loss: 2.308\n",
      "[7, 10701] loss: 2.304\n",
      "[7, 10751] loss: 2.306\n",
      "[7, 10801] loss: 2.301\n",
      "[7, 10851] loss: 2.308\n",
      "[7, 10901] loss: 2.307\n",
      "[7, 10951] loss: 2.305\n",
      "[7, 11001] loss: 2.305\n",
      "[7, 11051] loss: 2.298\n",
      "[7, 11101] loss: 2.310\n",
      "[7, 11151] loss: 2.309\n",
      "[7, 11201] loss: 2.296\n",
      "[7, 11251] loss: 2.301\n",
      "[7, 11301] loss: 2.304\n",
      "[7, 11351] loss: 2.304\n",
      "[7, 11401] loss: 2.300\n",
      "[7, 11451] loss: 2.305\n",
      "[7, 11501] loss: 2.303\n",
      "[7, 11551] loss: 2.309\n",
      "[7, 11601] loss: 2.309\n",
      "[7, 11651] loss: 2.300\n",
      "[7, 11701] loss: 2.300\n",
      "[7, 11751] loss: 2.312\n",
      "[7, 11801] loss: 2.310\n",
      "[7, 11851] loss: 2.300\n",
      "[7, 11901] loss: 2.300\n",
      "[7, 11951] loss: 2.307\n",
      "[7, 12001] loss: 2.304\n",
      "[7, 12051] loss: 2.308\n",
      "[7, 12101] loss: 2.306\n",
      "[7, 12151] loss: 2.304\n",
      "[7, 12201] loss: 2.303\n",
      "[7, 12251] loss: 2.302\n",
      "[7, 12301] loss: 2.300\n",
      "[7, 12351] loss: 2.304\n",
      "[7, 12401] loss: 2.303\n",
      "[7, 12451] loss: 2.306\n",
      "[8,     1] loss: 0.047\n",
      "[8,    51] loss: 2.302\n",
      "[8,   101] loss: 2.300\n",
      "[8,   151] loss: 2.307\n",
      "[8,   201] loss: 2.303\n",
      "[8,   251] loss: 2.301\n",
      "[8,   301] loss: 2.308\n",
      "[8,   351] loss: 2.298\n",
      "[8,   401] loss: 2.305\n",
      "[8,   451] loss: 2.299\n",
      "[8,   501] loss: 2.305\n",
      "[8,   551] loss: 2.298\n",
      "[8,   601] loss: 2.304\n",
      "[8,   651] loss: 2.305\n",
      "[8,   701] loss: 2.303\n",
      "[8,   751] loss: 2.307\n",
      "[8,   801] loss: 2.302\n",
      "[8,   851] loss: 2.301\n",
      "[8,   901] loss: 2.307\n",
      "[8,   951] loss: 2.300\n",
      "[8,  1001] loss: 2.307\n",
      "[8,  1051] loss: 2.300\n",
      "[8,  1101] loss: 2.304\n",
      "[8,  1151] loss: 2.305\n",
      "[8,  1201] loss: 2.298\n",
      "[8,  1251] loss: 2.309\n",
      "[8,  1301] loss: 2.300\n",
      "[8,  1351] loss: 2.307\n",
      "[8,  1401] loss: 2.308\n",
      "[8,  1451] loss: 2.305\n",
      "[8,  1501] loss: 2.300\n",
      "[8,  1551] loss: 2.306\n",
      "[8,  1601] loss: 2.307\n",
      "[8,  1651] loss: 2.303\n",
      "[8,  1701] loss: 2.302\n",
      "[8,  1751] loss: 2.309\n",
      "[8,  1801] loss: 2.302\n",
      "[8,  1851] loss: 2.303\n",
      "[8,  1901] loss: 2.307\n",
      "[8,  1951] loss: 2.300\n",
      "[8,  2001] loss: 2.303\n",
      "[8,  2051] loss: 2.298\n",
      "[8,  2101] loss: 2.306\n",
      "[8,  2151] loss: 2.306\n",
      "[8,  2201] loss: 2.306\n",
      "[8,  2251] loss: 2.310\n",
      "[8,  2301] loss: 2.301\n",
      "[8,  2351] loss: 2.303\n",
      "[8,  2401] loss: 2.309\n",
      "[8,  2451] loss: 2.301\n",
      "[8,  2501] loss: 2.303\n",
      "[8,  2551] loss: 2.304\n",
      "[8,  2601] loss: 2.308\n",
      "[8,  2651] loss: 2.307\n",
      "[8,  2701] loss: 2.303\n",
      "[8,  2751] loss: 2.308\n",
      "[8,  2801] loss: 2.306\n",
      "[8,  2851] loss: 2.309\n",
      "[8,  2901] loss: 2.308\n",
      "[8,  2951] loss: 2.297\n",
      "[8,  3001] loss: 2.304\n",
      "[8,  3051] loss: 2.307\n",
      "[8,  3101] loss: 2.302\n",
      "[8,  3151] loss: 2.300\n",
      "[8,  3201] loss: 2.300\n",
      "[8,  3251] loss: 2.304\n",
      "[8,  3301] loss: 2.309\n",
      "[8,  3351] loss: 2.296\n",
      "[8,  3401] loss: 2.299\n",
      "[8,  3451] loss: 2.302\n",
      "[8,  3501] loss: 2.309\n",
      "[8,  3551] loss: 2.307\n",
      "[8,  3601] loss: 2.303\n",
      "[8,  3651] loss: 2.306\n",
      "[8,  3701] loss: 2.301\n",
      "[8,  3751] loss: 2.304\n",
      "[8,  3801] loss: 2.305\n",
      "[8,  3851] loss: 2.304\n",
      "[8,  3901] loss: 2.305\n",
      "[8,  3951] loss: 2.305\n",
      "[8,  4001] loss: 2.302\n",
      "[8,  4051] loss: 2.306\n",
      "[8,  4101] loss: 2.305\n",
      "[8,  4151] loss: 2.302\n",
      "[8,  4201] loss: 2.303\n",
      "[8,  4251] loss: 2.303\n",
      "[8,  4301] loss: 2.306\n",
      "[8,  4351] loss: 2.303\n",
      "[8,  4401] loss: 2.303\n",
      "[8,  4451] loss: 2.303\n",
      "[8,  4501] loss: 2.306\n",
      "[8,  4551] loss: 2.306\n",
      "[8,  4601] loss: 2.305\n",
      "[8,  4651] loss: 2.306\n",
      "[8,  4701] loss: 2.309\n",
      "[8,  4751] loss: 2.305\n",
      "[8,  4801] loss: 2.302\n",
      "[8,  4851] loss: 2.311\n",
      "[8,  4901] loss: 2.302\n",
      "[8,  4951] loss: 2.301\n",
      "[8,  5001] loss: 2.301\n",
      "[8,  5051] loss: 2.303\n",
      "[8,  5101] loss: 2.303\n",
      "[8,  5151] loss: 2.305\n",
      "[8,  5201] loss: 2.299\n",
      "[8,  5251] loss: 2.305\n",
      "[8,  5301] loss: 2.308\n",
      "[8,  5351] loss: 2.297\n",
      "[8,  5401] loss: 2.300\n",
      "[8,  5451] loss: 2.303\n",
      "[8,  5501] loss: 2.305\n",
      "[8,  5551] loss: 2.302\n",
      "[8,  5601] loss: 2.303\n",
      "[8,  5651] loss: 2.303\n",
      "[8,  5701] loss: 2.303\n",
      "[8,  5751] loss: 2.311\n",
      "[8,  5801] loss: 2.306\n",
      "[8,  5851] loss: 2.300\n",
      "[8,  5901] loss: 2.303\n",
      "[8,  5951] loss: 2.308\n",
      "[8,  6001] loss: 2.303\n",
      "[8,  6051] loss: 2.303\n",
      "[8,  6101] loss: 2.301\n",
      "[8,  6151] loss: 2.306\n",
      "[8,  6201] loss: 2.301\n",
      "[8,  6251] loss: 2.301\n",
      "[8,  6301] loss: 2.305\n",
      "[8,  6351] loss: 2.300\n",
      "[8,  6401] loss: 2.302\n",
      "[8,  6451] loss: 2.302\n",
      "[8,  6501] loss: 2.305\n",
      "[8,  6551] loss: 2.304\n",
      "[8,  6601] loss: 2.302\n",
      "[8,  6651] loss: 2.295\n",
      "[8,  6701] loss: 2.307\n",
      "[8,  6751] loss: 2.303\n",
      "[8,  6801] loss: 2.304\n",
      "[8,  6851] loss: 2.306\n",
      "[8,  6901] loss: 2.302\n",
      "[8,  6951] loss: 2.307\n",
      "[8,  7001] loss: 2.303\n",
      "[8,  7051] loss: 2.300\n",
      "[8,  7101] loss: 2.308\n",
      "[8,  7151] loss: 2.305\n",
      "[8,  7201] loss: 2.306\n",
      "[8,  7251] loss: 2.303\n",
      "[8,  7301] loss: 2.305\n",
      "[8,  7351] loss: 2.297\n",
      "[8,  7401] loss: 2.305\n",
      "[8,  7451] loss: 2.304\n",
      "[8,  7501] loss: 2.299\n",
      "[8,  7551] loss: 2.303\n",
      "[8,  7601] loss: 2.301\n",
      "[8,  7651] loss: 2.304\n",
      "[8,  7701] loss: 2.307\n",
      "[8,  7751] loss: 2.300\n",
      "[8,  7801] loss: 2.296\n",
      "[8,  7851] loss: 2.309\n",
      "[8,  7901] loss: 2.310\n",
      "[8,  7951] loss: 2.301\n",
      "[8,  8001] loss: 2.303\n",
      "[8,  8051] loss: 2.300\n",
      "[8,  8101] loss: 2.304\n",
      "[8,  8151] loss: 2.304\n",
      "[8,  8201] loss: 2.303\n",
      "[8,  8251] loss: 2.305\n",
      "[8,  8301] loss: 2.299\n",
      "[8,  8351] loss: 2.307\n",
      "[8,  8401] loss: 2.297\n",
      "[8,  8451] loss: 2.301\n",
      "[8,  8501] loss: 2.304\n",
      "[8,  8551] loss: 2.307\n",
      "[8,  8601] loss: 2.303\n",
      "[8,  8651] loss: 2.303\n",
      "[8,  8701] loss: 2.300\n",
      "[8,  8751] loss: 2.303\n",
      "[8,  8801] loss: 2.304\n",
      "[8,  8851] loss: 2.304\n",
      "[8,  8901] loss: 2.307\n",
      "[8,  8951] loss: 2.299\n",
      "[8,  9001] loss: 2.302\n",
      "[8,  9051] loss: 2.298\n",
      "[8,  9101] loss: 2.310\n",
      "[8,  9151] loss: 2.309\n",
      "[8,  9201] loss: 2.304\n",
      "[8,  9251] loss: 2.303\n",
      "[8,  9301] loss: 2.301\n",
      "[8,  9351] loss: 2.309\n",
      "[8,  9401] loss: 2.301\n",
      "[8,  9451] loss: 2.300\n",
      "[8,  9501] loss: 2.297\n",
      "[8,  9551] loss: 2.304\n",
      "[8,  9601] loss: 2.303\n",
      "[8,  9651] loss: 2.303\n",
      "[8,  9701] loss: 2.309\n",
      "[8,  9751] loss: 2.305\n",
      "[8,  9801] loss: 2.305\n",
      "[8,  9851] loss: 2.300\n",
      "[8,  9901] loss: 2.308\n",
      "[8,  9951] loss: 2.305\n",
      "[8, 10001] loss: 2.304\n",
      "[8, 10051] loss: 2.302\n",
      "[8, 10101] loss: 2.304\n",
      "[8, 10151] loss: 2.305\n",
      "[8, 10201] loss: 2.300\n",
      "[8, 10251] loss: 2.302\n",
      "[8, 10301] loss: 2.302\n",
      "[8, 10351] loss: 2.301\n",
      "[8, 10401] loss: 2.307\n",
      "[8, 10451] loss: 2.304\n",
      "[8, 10501] loss: 2.306\n",
      "[8, 10551] loss: 2.305\n",
      "[8, 10601] loss: 2.303\n",
      "[8, 10651] loss: 2.305\n",
      "[8, 10701] loss: 2.305\n",
      "[8, 10751] loss: 2.297\n",
      "[8, 10801] loss: 2.299\n",
      "[8, 10851] loss: 2.308\n",
      "[8, 10901] loss: 2.301\n",
      "[8, 10951] loss: 2.303\n",
      "[8, 11001] loss: 2.310\n",
      "[8, 11051] loss: 2.307\n",
      "[8, 11101] loss: 2.307\n",
      "[8, 11151] loss: 2.303\n",
      "[8, 11201] loss: 2.304\n",
      "[8, 11251] loss: 2.304\n",
      "[8, 11301] loss: 2.301\n",
      "[8, 11351] loss: 2.305\n",
      "[8, 11401] loss: 2.305\n",
      "[8, 11451] loss: 2.299\n",
      "[8, 11501] loss: 2.305\n",
      "[8, 11551] loss: 2.305\n",
      "[8, 11601] loss: 2.311\n",
      "[8, 11651] loss: 2.303\n",
      "[8, 11701] loss: 2.304\n",
      "[8, 11751] loss: 2.302\n",
      "[8, 11801] loss: 2.303\n",
      "[8, 11851] loss: 2.300\n",
      "[8, 11901] loss: 2.302\n",
      "[8, 11951] loss: 2.303\n",
      "[8, 12001] loss: 2.298\n",
      "[8, 12051] loss: 2.310\n",
      "[8, 12101] loss: 2.305\n",
      "[8, 12151] loss: 2.301\n",
      "[8, 12201] loss: 2.300\n",
      "[8, 12251] loss: 2.299\n",
      "[8, 12301] loss: 2.306\n",
      "[8, 12351] loss: 2.305\n",
      "[8, 12401] loss: 2.303\n",
      "[8, 12451] loss: 2.304\n",
      "[9,     1] loss: 0.046\n",
      "[9,    51] loss: 2.302\n",
      "[9,   101] loss: 2.305\n",
      "[9,   151] loss: 2.304\n",
      "[9,   201] loss: 2.310\n",
      "[9,   251] loss: 2.301\n",
      "[9,   301] loss: 2.305\n",
      "[9,   351] loss: 2.305\n",
      "[9,   401] loss: 2.299\n",
      "[9,   451] loss: 2.306\n",
      "[9,   501] loss: 2.302\n",
      "[9,   551] loss: 2.304\n",
      "[9,   601] loss: 2.300\n",
      "[9,   651] loss: 2.307\n",
      "[9,   701] loss: 2.303\n",
      "[9,   751] loss: 2.301\n",
      "[9,   801] loss: 2.304\n",
      "[9,   851] loss: 2.306\n",
      "[9,   901] loss: 2.306\n",
      "[9,   951] loss: 2.302\n",
      "[9,  1001] loss: 2.303\n",
      "[9,  1051] loss: 2.309\n",
      "[9,  1101] loss: 2.308\n",
      "[9,  1151] loss: 2.301\n",
      "[9,  1201] loss: 2.305\n",
      "[9,  1251] loss: 2.307\n",
      "[9,  1301] loss: 2.304\n",
      "[9,  1351] loss: 2.303\n",
      "[9,  1401] loss: 2.298\n",
      "[9,  1451] loss: 2.311\n",
      "[9,  1501] loss: 2.303\n",
      "[9,  1551] loss: 2.301\n",
      "[9,  1601] loss: 2.302\n",
      "[9,  1651] loss: 2.299\n",
      "[9,  1701] loss: 2.304\n",
      "[9,  1751] loss: 2.308\n",
      "[9,  1801] loss: 2.301\n",
      "[9,  1851] loss: 2.305\n",
      "[9,  1901] loss: 2.301\n",
      "[9,  1951] loss: 2.301\n",
      "[9,  2001] loss: 2.308\n",
      "[9,  2051] loss: 2.303\n",
      "[9,  2101] loss: 2.308\n",
      "[9,  2151] loss: 2.305\n",
      "[9,  2201] loss: 2.312\n",
      "[9,  2251] loss: 2.300\n",
      "[9,  2301] loss: 2.305\n",
      "[9,  2351] loss: 2.305\n",
      "[9,  2401] loss: 2.306\n",
      "[9,  2451] loss: 2.306\n",
      "[9,  2501] loss: 2.304\n",
      "[9,  2551] loss: 2.305\n",
      "[9,  2601] loss: 2.304\n",
      "[9,  2651] loss: 2.305\n",
      "[9,  2701] loss: 2.303\n",
      "[9,  2751] loss: 2.306\n",
      "[9,  2801] loss: 2.304\n",
      "[9,  2851] loss: 2.300\n",
      "[9,  2901] loss: 2.298\n",
      "[9,  2951] loss: 2.303\n",
      "[9,  3001] loss: 2.304\n",
      "[9,  3051] loss: 2.306\n",
      "[9,  3101] loss: 2.305\n",
      "[9,  3151] loss: 2.302\n",
      "[9,  3201] loss: 2.303\n",
      "[9,  3251] loss: 2.308\n",
      "[9,  3301] loss: 2.302\n",
      "[9,  3351] loss: 2.304\n",
      "[9,  3401] loss: 2.297\n",
      "[9,  3451] loss: 2.299\n",
      "[9,  3501] loss: 2.312\n",
      "[9,  3551] loss: 2.301\n",
      "[9,  3601] loss: 2.306\n",
      "[9,  3651] loss: 2.300\n",
      "[9,  3701] loss: 2.306\n",
      "[9,  3751] loss: 2.308\n",
      "[9,  3801] loss: 2.299\n",
      "[9,  3851] loss: 2.301\n",
      "[9,  3901] loss: 2.309\n",
      "[9,  3951] loss: 2.306\n",
      "[9,  4001] loss: 2.305\n",
      "[9,  4051] loss: 2.307\n",
      "[9,  4101] loss: 2.306\n",
      "[9,  4151] loss: 2.304\n",
      "[9,  4201] loss: 2.299\n",
      "[9,  4251] loss: 2.303\n",
      "[9,  4301] loss: 2.306\n",
      "[9,  4351] loss: 2.307\n",
      "[9,  4401] loss: 2.299\n",
      "[9,  4451] loss: 2.304\n",
      "[9,  4501] loss: 2.309\n",
      "[9,  4551] loss: 2.302\n",
      "[9,  4601] loss: 2.305\n",
      "[9,  4651] loss: 2.300\n",
      "[9,  4701] loss: 2.302\n",
      "[9,  4751] loss: 2.306\n",
      "[9,  4801] loss: 2.299\n",
      "[9,  4851] loss: 2.304\n",
      "[9,  4901] loss: 2.293\n",
      "[9,  4951] loss: 2.303\n",
      "[9,  5001] loss: 2.301\n",
      "[9,  5051] loss: 2.302\n",
      "[9,  5101] loss: 2.304\n",
      "[9,  5151] loss: 2.302\n",
      "[9,  5201] loss: 2.301\n",
      "[9,  5251] loss: 2.304\n",
      "[9,  5301] loss: 2.309\n",
      "[9,  5351] loss: 2.307\n",
      "[9,  5401] loss: 2.298\n",
      "[9,  5451] loss: 2.300\n",
      "[9,  5501] loss: 2.305\n",
      "[9,  5551] loss: 2.303\n",
      "[9,  5601] loss: 2.304\n",
      "[9,  5651] loss: 2.305\n",
      "[9,  5701] loss: 2.306\n",
      "[9,  5751] loss: 2.307\n",
      "[9,  5801] loss: 2.303\n",
      "[9,  5851] loss: 2.307\n",
      "[9,  5901] loss: 2.309\n",
      "[9,  5951] loss: 2.305\n",
      "[9,  6001] loss: 2.299\n",
      "[9,  6051] loss: 2.301\n",
      "[9,  6101] loss: 2.303\n",
      "[9,  6151] loss: 2.303\n",
      "[9,  6201] loss: 2.296\n",
      "[9,  6251] loss: 2.299\n",
      "[9,  6301] loss: 2.305\n",
      "[9,  6351] loss: 2.301\n",
      "[9,  6401] loss: 2.310\n",
      "[9,  6451] loss: 2.300\n",
      "[9,  6501] loss: 2.304\n",
      "[9,  6551] loss: 2.297\n",
      "[9,  6601] loss: 2.305\n",
      "[9,  6651] loss: 2.304\n",
      "[9,  6701] loss: 2.305\n",
      "[9,  6751] loss: 2.303\n",
      "[9,  6801] loss: 2.300\n",
      "[9,  6851] loss: 2.300\n",
      "[9,  6901] loss: 2.296\n",
      "[9,  6951] loss: 2.304\n",
      "[9,  7001] loss: 2.307\n",
      "[9,  7051] loss: 2.302\n",
      "[9,  7101] loss: 2.303\n",
      "[9,  7151] loss: 2.302\n",
      "[9,  7201] loss: 2.302\n",
      "[9,  7251] loss: 2.304\n",
      "[9,  7301] loss: 2.306\n",
      "[9,  7351] loss: 2.305\n",
      "[9,  7401] loss: 2.303\n",
      "[9,  7451] loss: 2.308\n",
      "[9,  7501] loss: 2.298\n",
      "[9,  7551] loss: 2.303\n",
      "[9,  7601] loss: 2.303\n",
      "[9,  7651] loss: 2.307\n",
      "[9,  7701] loss: 2.299\n",
      "[9,  7751] loss: 2.301\n",
      "[9,  7801] loss: 2.304\n",
      "[9,  7851] loss: 2.304\n",
      "[9,  7901] loss: 2.308\n",
      "[9,  7951] loss: 2.307\n",
      "[9,  8001] loss: 2.304\n",
      "[9,  8051] loss: 2.302\n",
      "[9,  8101] loss: 2.303\n",
      "[9,  8151] loss: 2.304\n",
      "[9,  8201] loss: 2.301\n",
      "[9,  8251] loss: 2.310\n",
      "[9,  8301] loss: 2.306\n",
      "[9,  8351] loss: 2.305\n",
      "[9,  8401] loss: 2.298\n",
      "[9,  8451] loss: 2.305\n",
      "[9,  8501] loss: 2.300\n",
      "[9,  8551] loss: 2.304\n",
      "[9,  8601] loss: 2.301\n",
      "[9,  8651] loss: 2.312\n",
      "[9,  8701] loss: 2.301\n",
      "[9,  8751] loss: 2.304\n",
      "[9,  8801] loss: 2.303\n",
      "[9,  8851] loss: 2.300\n",
      "[9,  8901] loss: 2.301\n",
      "[9,  8951] loss: 2.298\n",
      "[9,  9001] loss: 2.304\n",
      "[9,  9051] loss: 2.303\n",
      "[9,  9101] loss: 2.312\n",
      "[9,  9151] loss: 2.306\n",
      "[9,  9201] loss: 2.305\n",
      "[9,  9251] loss: 2.303\n",
      "[9,  9301] loss: 2.301\n",
      "[9,  9351] loss: 2.308\n",
      "[9,  9401] loss: 2.304\n",
      "[9,  9451] loss: 2.307\n",
      "[9,  9501] loss: 2.304\n",
      "[9,  9551] loss: 2.306\n",
      "[9,  9601] loss: 2.304\n",
      "[9,  9651] loss: 2.297\n",
      "[9,  9701] loss: 2.303\n",
      "[9,  9751] loss: 2.300\n",
      "[9,  9801] loss: 2.306\n",
      "[9,  9851] loss: 2.308\n",
      "[9,  9901] loss: 2.305\n",
      "[9,  9951] loss: 2.307\n",
      "[9, 10001] loss: 2.303\n",
      "[9, 10051] loss: 2.305\n",
      "[9, 10101] loss: 2.304\n",
      "[9, 10151] loss: 2.304\n",
      "[9, 10201] loss: 2.302\n",
      "[9, 10251] loss: 2.307\n",
      "[9, 10301] loss: 2.304\n",
      "[9, 10351] loss: 2.305\n",
      "[9, 10401] loss: 2.303\n",
      "[9, 10451] loss: 2.303\n",
      "[9, 10501] loss: 2.305\n",
      "[9, 10551] loss: 2.304\n",
      "[9, 10601] loss: 2.305\n",
      "[9, 10651] loss: 2.301\n",
      "[9, 10701] loss: 2.302\n",
      "[9, 10751] loss: 2.300\n",
      "[9, 10801] loss: 2.307\n",
      "[9, 10851] loss: 2.306\n",
      "[9, 10901] loss: 2.307\n",
      "[9, 10951] loss: 2.310\n",
      "[9, 11001] loss: 2.300\n",
      "[9, 11051] loss: 2.299\n",
      "[9, 11101] loss: 2.302\n",
      "[9, 11151] loss: 2.303\n",
      "[9, 11201] loss: 2.301\n",
      "[9, 11251] loss: 2.304\n",
      "[9, 11301] loss: 2.307\n",
      "[9, 11351] loss: 2.296\n",
      "[9, 11401] loss: 2.304\n",
      "[9, 11451] loss: 2.306\n",
      "[9, 11501] loss: 2.302\n",
      "[9, 11551] loss: 2.299\n",
      "[9, 11601] loss: 2.302\n",
      "[9, 11651] loss: 2.304\n",
      "[9, 11701] loss: 2.301\n",
      "[9, 11751] loss: 2.301\n",
      "[9, 11801] loss: 2.305\n",
      "[9, 11851] loss: 2.308\n",
      "[9, 11901] loss: 2.301\n",
      "[9, 11951] loss: 2.302\n",
      "[9, 12001] loss: 2.310\n",
      "[9, 12051] loss: 2.305\n",
      "[9, 12101] loss: 2.305\n",
      "[9, 12151] loss: 2.309\n",
      "[9, 12201] loss: 2.302\n",
      "[9, 12251] loss: 2.305\n",
      "[9, 12301] loss: 2.299\n",
      "[9, 12351] loss: 2.304\n",
      "[9, 12401] loss: 2.302\n",
      "[9, 12451] loss: 2.305\n",
      "[10,     1] loss: 0.046\n",
      "[10,    51] loss: 2.311\n",
      "[10,   101] loss: 2.302\n",
      "[10,   151] loss: 2.310\n",
      "[10,   201] loss: 2.307\n",
      "[10,   251] loss: 2.304\n",
      "[10,   301] loss: 2.300\n",
      "[10,   351] loss: 2.304\n",
      "[10,   401] loss: 2.303\n",
      "[10,   451] loss: 2.308\n",
      "[10,   501] loss: 2.304\n",
      "[10,   551] loss: 2.304\n",
      "[10,   601] loss: 2.305\n",
      "[10,   651] loss: 2.300\n",
      "[10,   701] loss: 2.306\n",
      "[10,   751] loss: 2.304\n",
      "[10,   801] loss: 2.305\n",
      "[10,   851] loss: 2.301\n",
      "[10,   901] loss: 2.308\n",
      "[10,   951] loss: 2.304\n",
      "[10,  1001] loss: 2.301\n",
      "[10,  1051] loss: 2.298\n",
      "[10,  1101] loss: 2.302\n",
      "[10,  1151] loss: 2.299\n",
      "[10,  1201] loss: 2.301\n",
      "[10,  1251] loss: 2.300\n",
      "[10,  1301] loss: 2.307\n",
      "[10,  1351] loss: 2.306\n",
      "[10,  1401] loss: 2.307\n",
      "[10,  1451] loss: 2.304\n",
      "[10,  1501] loss: 2.298\n",
      "[10,  1551] loss: 2.307\n",
      "[10,  1601] loss: 2.302\n",
      "[10,  1651] loss: 2.304\n",
      "[10,  1701] loss: 2.306\n",
      "[10,  1751] loss: 2.298\n",
      "[10,  1801] loss: 2.300\n",
      "[10,  1851] loss: 2.304\n",
      "[10,  1901] loss: 2.305\n",
      "[10,  1951] loss: 2.304\n",
      "[10,  2001] loss: 2.299\n",
      "[10,  2051] loss: 2.299\n",
      "[10,  2101] loss: 2.308\n",
      "[10,  2151] loss: 2.304\n",
      "[10,  2201] loss: 2.301\n",
      "[10,  2251] loss: 2.306\n",
      "[10,  2301] loss: 2.304\n",
      "[10,  2351] loss: 2.309\n",
      "[10,  2401] loss: 2.303\n",
      "[10,  2451] loss: 2.299\n",
      "[10,  2501] loss: 2.304\n",
      "[10,  2551] loss: 2.305\n",
      "[10,  2601] loss: 2.308\n",
      "[10,  2651] loss: 2.301\n",
      "[10,  2701] loss: 2.303\n",
      "[10,  2751] loss: 2.311\n",
      "[10,  2801] loss: 2.302\n",
      "[10,  2851] loss: 2.304\n",
      "[10,  2901] loss: 2.305\n",
      "[10,  2951] loss: 2.307\n",
      "[10,  3001] loss: 2.308\n",
      "[10,  3051] loss: 2.308\n",
      "[10,  3101] loss: 2.306\n",
      "[10,  3151] loss: 2.304\n",
      "[10,  3201] loss: 2.300\n",
      "[10,  3251] loss: 2.304\n",
      "[10,  3301] loss: 2.297\n",
      "[10,  3351] loss: 2.307\n",
      "[10,  3401] loss: 2.303\n",
      "[10,  3451] loss: 2.303\n",
      "[10,  3501] loss: 2.302\n",
      "[10,  3551] loss: 2.303\n",
      "[10,  3601] loss: 2.303\n",
      "[10,  3651] loss: 2.301\n",
      "[10,  3701] loss: 2.303\n",
      "[10,  3751] loss: 2.302\n",
      "[10,  3801] loss: 2.304\n",
      "[10,  3851] loss: 2.299\n",
      "[10,  3901] loss: 2.304\n",
      "[10,  3951] loss: 2.301\n",
      "[10,  4001] loss: 2.308\n",
      "[10,  4051] loss: 2.307\n",
      "[10,  4101] loss: 2.302\n",
      "[10,  4151] loss: 2.303\n",
      "[10,  4201] loss: 2.306\n",
      "[10,  4251] loss: 2.301\n",
      "[10,  4301] loss: 2.305\n",
      "[10,  4351] loss: 2.307\n",
      "[10,  4401] loss: 2.303\n",
      "[10,  4451] loss: 2.306\n",
      "[10,  4501] loss: 2.306\n",
      "[10,  4551] loss: 2.304\n",
      "[10,  4601] loss: 2.299\n",
      "[10,  4651] loss: 2.300\n",
      "[10,  4701] loss: 2.308\n",
      "[10,  4751] loss: 2.302\n",
      "[10,  4801] loss: 2.303\n",
      "[10,  4851] loss: 2.304\n",
      "[10,  4901] loss: 2.303\n",
      "[10,  4951] loss: 2.300\n",
      "[10,  5001] loss: 2.301\n",
      "[10,  5051] loss: 2.302\n",
      "[10,  5101] loss: 2.305\n",
      "[10,  5151] loss: 2.305\n",
      "[10,  5201] loss: 2.300\n",
      "[10,  5251] loss: 2.297\n",
      "[10,  5301] loss: 2.305\n",
      "[10,  5351] loss: 2.298\n",
      "[10,  5401] loss: 2.306\n",
      "[10,  5451] loss: 2.302\n",
      "[10,  5501] loss: 2.303\n",
      "[10,  5551] loss: 2.305\n",
      "[10,  5601] loss: 2.309\n",
      "[10,  5651] loss: 2.306\n",
      "[10,  5701] loss: 2.302\n",
      "[10,  5751] loss: 2.309\n",
      "[10,  5801] loss: 2.298\n",
      "[10,  5851] loss: 2.298\n",
      "[10,  5901] loss: 2.304\n",
      "[10,  5951] loss: 2.304\n",
      "[10,  6001] loss: 2.302\n",
      "[10,  6051] loss: 2.302\n",
      "[10,  6101] loss: 2.307\n",
      "[10,  6151] loss: 2.304\n",
      "[10,  6201] loss: 2.305\n",
      "[10,  6251] loss: 2.302\n",
      "[10,  6301] loss: 2.304\n",
      "[10,  6351] loss: 2.302\n",
      "[10,  6401] loss: 2.306\n",
      "[10,  6451] loss: 2.309\n",
      "[10,  6501] loss: 2.300\n",
      "[10,  6551] loss: 2.301\n",
      "[10,  6601] loss: 2.306\n",
      "[10,  6651] loss: 2.306\n",
      "[10,  6701] loss: 2.297\n",
      "[10,  6751] loss: 2.304\n",
      "[10,  6801] loss: 2.304\n",
      "[10,  6851] loss: 2.304\n",
      "[10,  6901] loss: 2.306\n",
      "[10,  6951] loss: 2.299\n",
      "[10,  7001] loss: 2.309\n",
      "[10,  7051] loss: 2.306\n",
      "[10,  7101] loss: 2.301\n",
      "[10,  7151] loss: 2.304\n",
      "[10,  7201] loss: 2.309\n",
      "[10,  7251] loss: 2.301\n",
      "[10,  7301] loss: 2.304\n",
      "[10,  7351] loss: 2.299\n",
      "[10,  7401] loss: 2.303\n",
      "[10,  7451] loss: 2.303\n",
      "[10,  7501] loss: 2.306\n",
      "[10,  7551] loss: 2.304\n",
      "[10,  7601] loss: 2.304\n",
      "[10,  7651] loss: 2.301\n",
      "[10,  7701] loss: 2.303\n",
      "[10,  7751] loss: 2.301\n",
      "[10,  7801] loss: 2.303\n",
      "[10,  7851] loss: 2.305\n",
      "[10,  7901] loss: 2.303\n",
      "[10,  7951] loss: 2.298\n",
      "[10,  8001] loss: 2.301\n",
      "[10,  8051] loss: 2.301\n",
      "[10,  8101] loss: 2.302\n",
      "[10,  8151] loss: 2.295\n",
      "[10,  8201] loss: 2.306\n",
      "[10,  8251] loss: 2.302\n",
      "[10,  8301] loss: 2.307\n",
      "[10,  8351] loss: 2.302\n",
      "[10,  8401] loss: 2.303\n",
      "[10,  8451] loss: 2.302\n",
      "[10,  8501] loss: 2.309\n",
      "[10,  8551] loss: 2.306\n",
      "[10,  8601] loss: 2.304\n",
      "[10,  8651] loss: 2.311\n",
      "[10,  8701] loss: 2.306\n",
      "[10,  8751] loss: 2.303\n",
      "[10,  8801] loss: 2.302\n",
      "[10,  8851] loss: 2.303\n",
      "[10,  8901] loss: 2.303\n",
      "[10,  8951] loss: 2.310\n",
      "[10,  9001] loss: 2.305\n",
      "[10,  9051] loss: 2.303\n",
      "[10,  9101] loss: 2.301\n",
      "[10,  9151] loss: 2.303\n",
      "[10,  9201] loss: 2.308\n",
      "[10,  9251] loss: 2.305\n",
      "[10,  9301] loss: 2.302\n",
      "[10,  9351] loss: 2.303\n",
      "[10,  9401] loss: 2.302\n",
      "[10,  9451] loss: 2.310\n",
      "[10,  9501] loss: 2.303\n",
      "[10,  9551] loss: 2.306\n",
      "[10,  9601] loss: 2.302\n",
      "[10,  9651] loss: 2.302\n",
      "[10,  9701] loss: 2.303\n",
      "[10,  9751] loss: 2.310\n",
      "[10,  9801] loss: 2.307\n",
      "[10,  9851] loss: 2.307\n",
      "[10,  9901] loss: 2.299\n",
      "[10,  9951] loss: 2.298\n",
      "[10, 10001] loss: 2.309\n",
      "[10, 10051] loss: 2.309\n",
      "[10, 10101] loss: 2.303\n",
      "[10, 10151] loss: 2.305\n",
      "[10, 10201] loss: 2.304\n",
      "[10, 10251] loss: 2.309\n",
      "[10, 10301] loss: 2.304\n",
      "[10, 10351] loss: 2.302\n",
      "[10, 10401] loss: 2.303\n",
      "[10, 10451] loss: 2.309\n",
      "[10, 10501] loss: 2.301\n",
      "[10, 10551] loss: 2.301\n",
      "[10, 10601] loss: 2.303\n",
      "[10, 10651] loss: 2.305\n",
      "[10, 10701] loss: 2.305\n",
      "[10, 10751] loss: 2.308\n",
      "[10, 10801] loss: 2.304\n",
      "[10, 10851] loss: 2.302\n",
      "[10, 10901] loss: 2.300\n",
      "[10, 10951] loss: 2.306\n",
      "[10, 11001] loss: 2.298\n",
      "[10, 11051] loss: 2.308\n",
      "[10, 11101] loss: 2.303\n",
      "[10, 11151] loss: 2.308\n",
      "[10, 11201] loss: 2.298\n",
      "[10, 11251] loss: 2.303\n",
      "[10, 11301] loss: 2.309\n",
      "[10, 11351] loss: 2.295\n",
      "[10, 11401] loss: 2.313\n",
      "[10, 11451] loss: 2.303\n",
      "[10, 11501] loss: 2.302\n",
      "[10, 11551] loss: 2.302\n",
      "[10, 11601] loss: 2.300\n",
      "[10, 11651] loss: 2.307\n",
      "[10, 11701] loss: 2.311\n",
      "[10, 11751] loss: 2.300\n",
      "[10, 11801] loss: 2.305\n",
      "[10, 11851] loss: 2.301\n",
      "[10, 11901] loss: 2.302\n",
      "[10, 11951] loss: 2.300\n",
      "[10, 12001] loss: 2.301\n",
      "[10, 12051] loss: 2.311\n",
      "[10, 12101] loss: 2.302\n",
      "[10, 12151] loss: 2.307\n",
      "[10, 12201] loss: 2.303\n",
      "[10, 12251] loss: 2.303\n",
      "[10, 12301] loss: 2.299\n",
      "[10, 12351] loss: 2.301\n",
      "[10, 12401] loss: 2.300\n",
      "[10, 12451] loss: 2.306\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Train the network\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = dnn(inputs.view(inputs.size(0), -1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  cat   ship  ship  plane\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = dnn(inputs.view(inputs.size(0), -1))\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: plane is 0.0 %\n",
      "Accuracy for class: car   is 0.0 %\n",
      "Accuracy for class: bird  is 0.0 %\n",
      "Accuracy for class: cat   is 0.0 %\n",
      "Accuracy for class: deer  is 0.0 %\n",
      "Accuracy for class: dog   is 100.0 %\n",
      "Accuracy for class: frog  is 0.0 %\n",
      "Accuracy for class: horse is 0.0 %\n",
      "Accuracy for class: ship  is 0.0 %\n",
      "Accuracy for class: truck is 0.0 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = dnn(inputs.view(inputs.size(0), -1))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  cat   ship  ship  plane\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, use_batchnorm=False):\n",
    "        super(DNN, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(20):\n",
    "            self.hidden_layers.append(nn.Sequential(\n",
    "                nn.Linear(100, 100),\n",
    "                nn.BatchNorm1d(100) if use_batchnorm else nn.Identity(),\n",
    "                Swish()\n",
    "            ))\n",
    "        self.output_layer = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "dnn_bn = DeepNeuralNetwork(input_size=32*32*3, hidden_size=100, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "substitute batch normalization with SELU (Scaled Exponential Linear Units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(20):\n",
    "            self.hidden_layers.append(nn.Sequential(\n",
    "                nn.Linear(100, 100),\n",
    "                nn.SELU()\n",
    "            ))\n",
    "        self.output_layer = nn.Linear(100, 10)\n",
    "\n",
    "        # Initialize the weights using LeCun normal initialization\n",
    "        for layer in self.hidden_layers:\n",
    "            nn.init.kaiming_normal_(layer[0].weight, nonlinearity='linear')\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
