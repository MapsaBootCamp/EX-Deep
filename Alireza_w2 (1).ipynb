{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29de728f-9934-421e-b1cf-8f85a9ed0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/1000, Loss: 1.655561923980713\n",
      "Epoch: 200/1000, Loss: 1.6499935388565063\n",
      "Epoch: 300/1000, Loss: 1.6447288990020752\n",
      "Epoch: 400/1000, Loss: 1.6397470235824585\n",
      "Epoch: 500/1000, Loss: 1.6350294351577759\n",
      "Epoch: 600/1000, Loss: 1.63051176071167\n",
      "Epoch: 700/1000, Loss: 1.6262112855911255\n",
      "Epoch: 800/1000, Loss: 1.6221052408218384\n",
      "Epoch: 900/1000, Loss: 1.618170976638794\n",
      "Epoch: 1000/1000, Loss: 1.6143827438354492\n",
      "Predictions: tensor([1, 0, 3, 3, 4, 4, 0, 3, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "label_size = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "# Create the neural network object\n",
    "model = NeuralNetwork(input_size, hidden_size, label_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Generate some dummy data for training\n",
    "train_data = torch.randn(100, input_size)\n",
    "train_labels = torch.randint(label_size, (100,))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(train_data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, train_labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the trained model\n",
    "test_data = torch.randn(10, input_size)\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_data)\n",
    "\n",
    "    # Print the predictions\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    print(\"Predictions:\", predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922d9dda-f1ab-42ff-86e1-a440348ab851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 4159559.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 2124740.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2591428.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 6113776.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformation to apply to the data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the training set\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the test set\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e646fdf9-0229-4e1a-ae21-3e6534515b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, output_size) )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d884bb26-c345-4680-aff3-61adc88a3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MLP(input_size=784, output_size=10)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15534471-b433-4d25-80d0-7f568bd5475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6523\n",
      "Epoch [2/10], Loss: 0.1708\n",
      "Epoch [3/10], Loss: 0.3548\n",
      "Epoch [4/10], Loss: 0.1606\n",
      "Epoch [5/10], Loss: 0.5073\n",
      "Epoch [6/10], Loss: 0.0545\n",
      "Epoch [7/10], Loss: 0.0935\n",
      "Epoch [8/10], Loss: 0.0449\n",
      "Epoch [9/10], Loss: 0.1079\n",
      "Epoch [10/10], Loss: 0.1270\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten the images\n",
    "        images = images.view(-1, 784)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss after every epoch\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4621009b-1d87-46d3-b9b4-236de6647ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.44%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 784)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6b628-a4e5-4af7-b99f-4710824a416f",
   "metadata": {},
   "source": [
    "Manually tune the hyperparameters, such as the learning rate, by experimenting with different values and observing the performance. You can also search for the optimal learning rate by using techniques like learning rate range test, where you gradually increase the learning rate and monitor the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb0ce57-5fea-4b51-9606-7938f6c85f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a6b98-be95-47f7-a5b2-d1e65e349019",
   "metadata": {},
   "source": [
    "In this coding exercise, you'll have an opportunity to explore the behavior of a deep neural network trained on the CIFAR10 image dataset. Follow the steps below:\n",
    "a. Construct a deep neural network (DNN) using 20 hidden layers, each comprising 100 neurons. To facilitate this exploration, employ the Swish activation function for each layer. Utilize nn.ModuleList to manage the layers effectively.\n",
    "\n",
    "b. Load the CIFAR10 dataset for training your network. Utilize the appropriate function, such as torchvision.datasets.CIFAR10. The dataset consists of 60,000 color images, with dimensions of 32×32 pixels. It is divided into 50,000 training samples and 10,000 testing samples. With 10 classes in the dataset, ensure that your network has a softmax output layer comprising 10 neurons. When modifying the model's architecture or hyperparameters, conduct a search to identify an appropriate learning rate. Implement early stopping during training and employ the Nadam optimization algorithm.\n",
    "\n",
    "c. Experiment by adding batch normalization to your network. Compare the learning curves obtained with and without batch normalization. Analyze whether the model converges faster with batch normalization and observe any improvements in its performance. Additionally, assess the impact of batch normalization on training speed.\n",
    "\n",
    "d. As an additional experiment, substitute batch normalization with SELU (Scaled Exponential Linear Units). Make the necessary adjustments to ensure the network self-normalizes. This involves standardizing the input features, initializing the network's weights using LeCun normal initialization (nn.init.kaiming_normal_), and ensuring that the DNN consists solely of dense layers. Observe the effects of utilizing SELU activation and self-normalization on the network's training stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fafd162d-10e7-436e-96a3-b126c8e840dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd79624-1266-418c-8bf8-a14b064f17b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
