{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have been given a partially implemented code for a feed-forward neural network using PyTorch. Your task is to complete the missing parts of the code to make it functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/1000, Loss: 1.598299503326416\n",
      "Epoch: 200/1000, Loss: 1.5951542854309082\n",
      "Epoch: 300/1000, Loss: 1.5920203924179077\n",
      "Epoch: 400/1000, Loss: 1.5886024236679077\n",
      "Epoch: 500/1000, Loss: 1.5853784084320068\n",
      "Epoch: 600/1000, Loss: 1.582106590270996\n",
      "Epoch: 700/1000, Loss: 1.5788977146148682\n",
      "Epoch: 800/1000, Loss: 1.5759257078170776\n",
      "Epoch: 900/1000, Loss: 1.5731148719787598\n",
      "Epoch: 1000/1000, Loss: 1.5705993175506592\n",
      "Predictions: tensor([0, 3, 2, 0, 0, 0, 0, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x) # Complete this line to pass the output of the first layer through the activation function\n",
    "        x = self.relu(x)# Complete this line to pass the output of the activation function through the second layer\n",
    "        return x\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "label_size = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "# Create the neural network object\n",
    "model = NeuralNetwork(input_size, hidden_size, label_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Generate some dummy data for training\n",
    "train_data = torch.randn(100, input_size)\n",
    "train_labels = torch.randint(label_size, (100,))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    # Complete this line to pass the training data through the model and obtain the predictions\n",
    "    running_loss= 0\n",
    "    last_loss= 0\n",
    "    model.train(True)\n",
    "    outputs = model(train_data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, train_labels)\n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the trained model\n",
    "test_data = torch.randn(10, input_size)\n",
    "with torch.no_grad():\n",
    "    # Complete this line to pass the test data through the model and obtain the predictions\n",
    "    test_outputs = model(test_data)\n",
    "    # Print the predictions\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    print(\"Predictions:\", predicted)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In this coding exercise, you need to implement the training of a deep MLP on the MNIST dataset using PyTorch and manually tune the hyperparameters. Follow the steps below to proceed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the MNIST dataset using torchvision.datasets.MNIST. The dataset contains handwritten digit images, and it can be easily accessed through PyTorch's torchvision module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "from torchvision import datasets, transforms\n",
    "train_dataset = datasets.MNIST(root= \"./data\", train = True, download = True, transform = transforms)\n",
    "test_dataset = datasets.MNIST(root = \"./data\", train = False, download = True, transform = transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define your deep MLP model. Specify the number of hidden layers, the number of neurons in each layer, and the activation function to be used. You can use the nn.Sequential container to stack the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_size[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_size) - 1):\n",
    "            layers.append(nn.Linear(hidden_size[i], hidden_size[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_size[-1], output_size))\n",
    "\n",
    "        # Create the model using nn.Sequential\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up the training loop and the hyperparameters. You can use the CrossEntropyLoss as the loss function and the Stochastic Gradient Descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 128]\n",
    "output_size = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Create deep MLP model\n",
    "model = MLP(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model by iterating over the training dataset for the specified number of epochs. Compute the loss, perform backpropagation, and update the model's parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Flatten the input images\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        # Flatten the input images\n",
    "        batch_inputs = batch_inputs.view(-1, 784)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss after each epoch\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the trained model on the test dataset and calculate the accuracy (Please take a moment to consider the code below!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m      6\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m784\u001b[39m)\n\u001b[0;32m      7\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = MLP(input_size, hidden_sizes, output_size)\n",
    "model.load_state_dict(torch.load('deep_mlp_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Load the MNIST test dataset\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]))\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 784)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Manually tune the hyperparameters, such as the learning rate, by experimenting with different values and observing the performance. You can also search for the optimal learning rate by using techniques like learning rate range test, where you gradually increase the learning rate and monitor the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In this coding exercise, you'll have an opportunity to explore the behavior of a deep neural network trained on the CIFAR10 image dataset. Follow the steps below:\n",
    "\n",
    "* a. Construct a deep neural network (DNN) using 20 hidden layers, each comprising 100 neurons. To facilitate this exploration, employ the Swish activation function for each layer. Utilize nn.ModuleList to manage the layers effectively.\n",
    "\n",
    "* b. Load the CIFAR10 dataset for training your network. Utilize the appropriate function, such as torchvision.datasets.CIFAR10. The dataset consists of 60,000 color images, with dimensions of 32×32 pixels. It is divided into 50,000 training samples and 10,000 testing samples. With 10 classes in the dataset, ensure that your network has a softmax output layer comprising 10 neurons. When modifying the model's architecture or hyperparameters, conduct a search to identify an appropriate learning rate. Implement early stopping during training and employ the Nadam optimization algorithm.\n",
    "\n",
    "* c. Experiment by adding batch normalization to your network. Compare the learning curves obtained with and without batch normalization. Analyze whether the model converges faster with batch normalization and observe any improvements in its performance. Additionally, assess the impact of batch normalization on training speed.\n",
    "\n",
    "* d. As an additional experiment, substitute batch normalization with SELU (Scaled Exponential Linear Units). Make the necessary adjustments to ensure the network self-normalizes. This involves standardizing the input features, initializing the network's weights using LeCun normal initialization (nn.init.kaiming_normal_), and ensuring that the DNN consists solely of dense layers. Observe the effects of utilizing SELU activation and self-normalization on the network's training stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.optim' has no attribute 'Nadam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Define the loss function and optimizer\u001b[39;00m\n\u001b[0;32m     47\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 48\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNadam\u001b[49m(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Training loop with early stopping\u001b[39;00m\n\u001b[0;32m     51\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.optim' has no attribute 'Nadam'"
     ]
    }
   ],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DeepNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(20):\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(Swish())\n",
    "            input_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Set up data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the deep neural network\n",
    "input_size = 32 * 32 * 3  # CIFAR10 image dimensions\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "\n",
    "model = DeepNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Use Swish activation for all layers\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Nadam(model.parameters())\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 50\n",
    "early_stop_counter = 5\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, input_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, input_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            val_outputs = model(images)\n",
    "            val_loss += criterion(val_outputs, labels).item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= early_stop_counter:\n",
    "                print(f'Early stopping after {epoch + 1} epochs.')\n",
    "                break\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Validation Loss: {val_loss}')\n",
    "\n",
    "\n",
    "# Experiment with batch normalization\n",
    "class DMLBN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DMLBN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(20):\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(Swish())\n",
    "            input_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the deep neural network with batch normalization\n",
    "model_with_bn = DMLBN(input_size, hidden_size, output_size)\n",
    "model_with_bn = model_with_bn.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_with_bn = optim.Nadam(model_with_bn.parameters())\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss_with_bn = float('inf')\n",
    "counter_with_bn = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_with_bn.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, input_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        optimizer_with_bn.zero_grad()\n",
    "        outputs = model_with_bn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_with_bn.step()\n",
    "\n",
    "    # Validation\n",
    "    model_with_bn.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, input_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            val_outputs = model_with_bn(images)\n",
    "            val_loss += criterion(val_outputs, labels).item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_loss_with_bn:\n",
    "            best_loss_with_bn = val_loss\n",
    "            counter_with_bn = 0\n",
    "        else:\n",
    "            counter_with_bn += 1\n",
    "            if counter_with_bn >= early_stop_counter:\n",
    "                print(f'Early stopping with batch normalization after {epoch + 1} epochs.')\n",
    "                break\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Validation Loss: {val_loss}')\n",
    "\n",
    "\n",
    "# Experiment with SELU activation and self-normalization\n",
    "class DMLSELU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DMLSELU, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(20):\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.SELU())\n",
    "            input_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the deep neural network with SELU activation\n",
    "model_with_selu = DMLSELU(input_size, hidden_size, output_size)\n",
    "model_with_selu = model_with_selu.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Initialize weights with LeCun normal initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "\n",
    "model_with_selu.apply(init_weights)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_with_selu = optim.Nadam(model_with_selu.parameters())\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss_with_selu = float('inf')\n",
    "counter_with_selu = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_with_selu.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, input_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        optimizer_with_selu.zero_grad()\n",
    "        outputs = model_with_selu(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_with_selu.step()\n",
    "\n",
    "    # Validation\n",
    "    model_with_selu.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, input_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            val_outputs = model_with_selu(images)\n",
    "            val_loss += criterion(val_outputs, labels).item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_loss_with_selu:\n",
    "            best_loss_with_selu = val_loss\n",
    "            counter_with_selu = 0\n",
    "        else:\n",
    "            counter_with_selu += 1\n",
    "            if counter_with_selu >= early_stop_counter:\n",
    "                print(f'Early stopping with SELU after {epoch + 1} epochs.')\n",
    "                break\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Validation Loss: {val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
